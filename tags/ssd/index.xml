<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>SSD - Tag - Xing Lin</title>
        <link>https://example.com/tags/ssd/</link>
        <description>SSD - Tag - Xing Lin</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>linxingnku@gmail.com (Xing Lin)</managingEditor>
            <webMaster>linxingnku@gmail.com (Xing Lin)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 28 Jun 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://example.com/tags/ssd/" rel="self" type="application/rss+xml" /><item>
    <title>SHRD: Improving Spatial Locality in Flash Storage Accesses by Sequentializing in Host and Randomizing in Device</title>
    <link>https://example.com/shrd/</link>
    <pubDate>Sun, 28 Jun 2020 00:00:00 &#43;0000</pubDate>
    <author>Xing Lin</author>
    <guid>https://example.com/shrd/</guid>
    <description><![CDATA[I really liked the paper. The idea is neat. The paper is also well written and the design and the new idea is clearly presented. A very good paper to read and learn from on how to write good papers.
Summary The key idea is to convert random small writes into large sequential writes at the device driver. This reduces data fragmentation and also reduces request handling overhead for the SSD, as it can combine multiple small write requests into fewer large write requests.]]></description>
</item>
<item>
    <title>How to Evaluate Research Ideas for SSDs</title>
    <link>https://example.com/ssd-evaluation-how/</link>
    <pubDate>Wed, 17 Jun 2020 00:00:00 &#43;0000</pubDate>
    <author>Xing Lin</author>
    <guid>https://example.com/ssd-evaluation-how/</guid>
    <description><![CDATA[This blog lists tools/simulators/platforms people used to evaluate SSD researches.
  MQSim
MQSIM FAST18, ATC19-Kim
  SSDSim SSDSim Microsoft Patch
[ATC12][atc12]
  FEMU
FEMU-fast18-short
  ]]></description>
</item>
<item>
    <title>Open-Channel SSD</title>
    <link>https://example.com/ocssd/</link>
    <pubDate>Tue, 13 Aug 2019 00:00:00 &#43;0000</pubDate>
    <author>Xing Lin</author>
    <guid>https://example.com/ocssd/</guid>
    <description><![CDATA[In Linux pblk implementation, a line corresponds to a chunk which is one erase block. A LUN is a PU. A group is a channel.
  Eventually, pblk calls pblk_submit_read() and pblk_submit_write() for reads/writes to the device.
  Read code path (from the media) in pblk
  1 2 3 4 5 6 7 8 9  /* pblk-read.c */ pblk_submit_read() pblk_read_ppalist_rq(); or pblk_read_rq(); pblk_lookup_l2p_seq(); // get physical addresses pblk_submit_io(pblk, rqd); nvm_submit_io(dev, rqd); dev-&gt;ops-&gt;submit_io(dev, rqd);    Write code path in pblk  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  pblk_write_to_cache() { for (i = 0; i &lt; nr_entries; i++) { void *data = bio_data(bio); w_ctx.]]></description>
</item>
<item>
    <title>Coordinating Garbage Collection for Arrays of Solid-State Drives</title>
    <link>https://example.com/global-garbage-collection/</link>
    <pubDate>Fri, 08 Mar 2019 00:00:00 &#43;0000</pubDate>
    <author>Xing Lin</author>
    <guid>https://example.com/global-garbage-collection/</guid>
    <description><![CDATA[This is the start of a series of blogs I plan to write about existing work related with garbage collection for SSDs.
The main idea of this paper is to run garbage collection during idle times to minimize the impact on foreground workloads. Garbage collection is scheduled to run simultaneously at all SSDs, to maximize the time window during which there is no garbage collection, and thus higher application performance. Figure 5 shows their approach and Figure 10 shows the effects.]]></description>
</item>
</channel>
</rss>
