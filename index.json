[{"categories":null,"content":"USENIX OSDI 2020","date":"2022-07-29","objectID":"/cachelib/","tags":["paper","NoSQL","AWS"],"title":"The CacheLib Caching Engine: Design and Experiences at Scale","uri":"/cachelib/"},{"categories":null,"content":" CacheLib is a general-purpose caching engine, designed based on experiences with a range of caching use cases at Facebook, that facilitates the easy development and maintenance of caches. CacheLib was first deployed at Facebook in 2017 and today powers over 70 services including CDN, storage, and application-data caches. All of these systems process millions of queries per second, cache working sets large enough to require using both flash and DRAM for caching, and must tolerate frequent restarts due to application updates, which are common in the Facebook production environment. systems built on CacheLib achieve peak throughputs of a million requests per second on a single production server and hit ratios between 60 and 90%. Because workloads at Facebook are less cacheable than is generally assumed, caches at Facebook require massive capacities to achieve acceptable hit ratios. Caching systems at Facebook often comprise large distributed systems where each node has hundreds of gigabytes of cache capacity. This makes the use of flash for caching attractive. CDN focuses on serving HTTP requests for static media objects such as photos, audio, and video chunks from servers in user proximity. Churn refers to the change in the working set due to the introduction of new keys and changes in popularity of existing keys over time. The popular YCSB [24] workload generator assumes that there is no churn, i.e., each key will remain equally popular throughout the benchmark. In Facebook production workloads, we find a high degree of churn. We define an object to be popular if it is among the 10% of objects that receive the most requests. Across all workloads, over two-thirds of popular objects in a given hour fall out of the top 10% after just one hour. Such high churn applies independent of which hour we use as the baseline, for different percentiles (e.g., top 25%), and with different time granularities (e.g., after 10 minutes, 50% of popular objects are no longer popular). For Storage and CDN , we find that 64KB and 128KB chunks, respectively, are very common, which result from dividing large objects into chunks. production caches frequently restart in order to pick up code fixes and updates. For example, 75% of Lookaside caches and 95% of CDN caches have an uptime less than 7 days. Even systems such as Storage and Social-Graph which have longer uptimes on average follow a regular monthly maintenance schedule which requires restarting cache processes. For example, under LRU, popular Items frequently compete to be reset to the most-recently-used (MRU) position, leading to lock contention and limit throughput. CacheLib adopts a simple solution to reduce contention: Items that were recently reset to the MRU position are not reset again for some time T [9, 87]. T is set to 60 seconds in most deployments. ","date":"2022-07-29","objectID":"/cachelib/:0:0","tags":["paper","NoSQL","AWS"],"title":"The CacheLib Caching Engine: Design and Experiences at Scale","uri":"/cachelib/"},{"categories":null,"content":"USENIX ATC 2022","date":"2022-07-16","objectID":"/dynamodb/","tags":["paper","NoSQL","AWS"],"title":"Amazon DynamoDB: A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service","uri":"/dynamodb/"},{"categories":null,"content":" Hundreds of thousands of customers rely on DynamoDB for its fundamental properties In 2021, during the 66-hour Amazon Prime Day shopping event, Amazon systems - including Alexa, the Amazon.com sites, and Amazon fulfillment centers, made trillions of API calls to DynamoDB, peaking at 89.2 million requests per second, while experiencing high availability with single-digit millisecond performance. For DynamoDB customers, consistent performance at any scale is often more important than median request service times because unexpectedly high latency requests can amplify through higher layers of applications that depend on DynamoDB and lead to a bad customer experience. DynamoDB offers an availability SLA of 99.99 for regular tables and 99.999 for global tables (where DynamoDB replicates across tables across multiple AWS Regions) a service-oriented architecture was adopted to encapsulate an application’s data behind service-level APIs that allowed sufficient decoupling to address tasks like reconfiguration without having to disrupt clients. Only the leader replica can serve write and strongly consistent read requests. we discovered that application workloads frequently have non-uniform access patterns both over time and over key ranges. When the request rate within a table is non-uniform, splitting a partition and dividing performance allocation proportionately can result in the hot portion of the partition having less available performance than it did before the split. The key observation that partitions had non-uniform access also led us to observe that not all partitions hosted by a storage node used their allocated throughput simultaneously. If a table experienced throttling and the table level throughput was not exceeded, then it would automatically increase (boost) the allocated throughput of the partitions of the table using a proportional control algorithm. adaptive capacity was also best-effort but eliminated over 99.99% of the throttling due to skewed access pattern. Bursting was only helpful for short-lived spikes in traffic and it was dependent on the node having throughput to support bursting. Adaptive capacity was reactive and kicked in only after throttling had been observed. This meant that the application using the table had already experienced brief pe- riod of unavailability. quick healing of impacted replication groups using log replicas ensures high durability of most recent writes. the data of the live replicas matches with a copy of a replica built offline using the archived write-ahead log entries. we have learned that continuous verification of data-at-rest is the most reliable method of protecting against hardware failures, silent data corruption, and even software bugs. We use formal methods [16] extensively to ensure the correctness of our replication protocols. The core replication protocol was specified using TLA+. Model checking has allowed us to catch subtle bugs that could have led to durability and correctness issues before the code went into production. formal methods have also been used to verify the correctness of our control plane and features such as distributed transactions. Backups or restores don’t affect performance or availability of the table as they are built using the write-ahead logs that are archived in S3. If one of the replicas is unresponsive, the leader adds a log replica to the group. Adding a log replica is the fastest way to ensure that the write quorum of the group is always met. We have been able to run millions of Paxos groups in a Region with log replicas. To solve the availability problem caused by gray failures, a follower that wants to trigger a failover sends a message to other replicas in the replication group asking if they can communicate with the leader. The rolled-back state might be different from the initial state of the software. The rollback procedure is often missed in testing and can lead to customer impact. DynamoDB runs a suite of upgrade and downgrade test","date":"2022-07-16","objectID":"/dynamodb/:0:0","tags":["paper","NoSQL","AWS"],"title":"Amazon DynamoDB: A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service","uri":"/dynamodb/"},{"categories":null,"content":"I finished reading Andrew Grove’s book on ``High Output Management’’ and really liked it, especially when reading the second half. It explains a few concepts, such as how a person’s needs may change and what is controlling a person’s behavior. It also contains quite a few practical advices on how to handle issues that could happen in a workplace. I highly recommend everyone to take a look at this book. Maslow’s hierarchy of needs: physiological -\u003e safety/security -\u003e social/affiliation -\u003e esteem/recognition -\u003e self actualization. When a lower need is satisfied, one higher is likely to take over. Models of control Free-market forces For simple situations. Everyone is openly serving his own self-interest. The goods/services have a very clearly defined dollar value. Example: buying a tire Contractual obligations Values are not so clearly defined (what is the contribution of a team member to the overal team/project? ). governed by a set of agreed-upon rules Example: stop at red signal; Employment contract Cultural values: The environment changes more rapidly than one change rules. When a set of circumstances is so ambiguous and unclear that a contract between parties that attempted to cover all possibilities would be prohibitively complicated. Example: stop your car to help accident. Money in the physiological- and security-driven modes only motivates until the need is satisfied, but money as a measure of achievement will motivate without limit. Comparing our work to sports may also teach us how to cope with failure. One of the big impediments to a fully committed, highly motivated state of mind is preoccupation with failure. Yet we know that in any competitive sport, at least 50 percent of all matches are lost. All participants know that from the outset, and yet rarely do they give up at any stage of a contest. The performance rating of a manager cannot be higher than the one we would accord to his organization. It is very important to assess actual performance, not appearances; real output, not good form. By elevating someone, we are, in effect, creating role models for others in our organization. The old saying has it that when we promote the good salesman and make him a manager, we ruin a good salesman and get a bad manager. Performance review is about giving suggestions on improving his performance. When giving feedbacks to your team members, the key is to recognize that everyone has only a finite capacity to deal with facts, issues, and suggestions. You may possess seven truths about his performance, but if his capacity is only four, at best you’ll waste your breath on the other three. At worst you will have left him with a case of sensory overload, and he will go away without getting anything out of the review. Stages of problem-solving: ignore -\u003e deny -\u003e blame others -\u003e assume responsibility -\u003e find solution. The critical part is to move from the “blame others” stage to the “assume responsibility” stage. We should spend more time trying to improve the performance of our stars. These people account for a disproportionately large share of the work in any organization. Put another way, concentrating on the starts is a high-leverage activity: if they get better, the impact on group output is very great indeed. Best interview questions Describe some projects that were highly regarded by your management, especially by the levels above your immediate supervisor What are your weaknesses? How are you working to eliminate them? Convince me why my company should hire you. What are some of the problems you are encountering in your current position? How are you going about solving them? What could you have done to prevent them from cropping up? Why do you think you are ready for this new job? What do you consdier your most significant achievements? Why were they important to you? What do you consider your most significant failures? What do you learn from them? Why do you think an engineer should be chosen for a marketing position? (very","date":"2021-06-13","objectID":"/high-output-management/:0:0","tags":["book review"],"title":"Book Review: High Output Management","uri":"/high-output-management/"},{"categories":null,"content":"YCSB Workload Description Example Operation breakdown A Update heavy workload session store recording recent actions read: 50%, update: 50% B read mostly workload photo tagging; add a tag is an update, but most operations are to read tags read: 95%, update: 5% C read only user profile cache, where profiles are constructed elsewhere )e.g., Hadoop) read: 100% D read latest workload user status updates; people want to read the latest read: 95%, insert 5% E short ranges threaded conversations, where each scan is for the posts in a given thread (assumed to be clustered by thread id) scan: 95% (maxscanlength=100), insert: 5 F read-modify-write workload user database, where user records are read and modified by the user or to record user activity. read: 50%, readmodifywrite: 50% RocksDB Workloads in Facebook paper link ","date":"2021-03-05","objectID":"/keyvalue-workloads/:0:0","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":"UDB Social graph data stored in MyRocks Get: 75%, Put: 20% 75+% of KV-pairs are Put only once. Most of the start-keys of Iterators are used only once. The scan length of more than 60% of the Iterators is only 1 across all CFs. ","date":"2021-03-05","objectID":"/keyvalue-workloads/:1:0","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":"ZippyDB Distributed KV store, mapping some metadata of an object to the object address in an object storage system 78% Get, 13% Put, 6% Delete, and 3% Iterator For about 80% of the KV-pairs, Get requests only occur once. A very small portion of KV-pairs have very large read counts over the 24-hour period. 1% of the KV-pairs show more than 100 Get requests, and the Gets to these KV-pairs are about 50% of the total Gets that show strong localities. About 73% of the KV-pairs are Put only once, and fewer than 0.001% of the KV-pairs are Put more than 10 times. Put does not have as clear a locality as Get does. ","date":"2021-03-05","objectID":"/keyvalue-workloads/:2:0","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":"UP2X statistic counters of user activities for AI/ML prediction and interference merge (read-modify-write): 92.53%, 7.46% Get Merge and Get have wide distributions of access counts. Most KV pairs are Put only once. ","date":"2021-03-05","objectID":"/keyvalue-workloads/:3:0","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":"UDB Request Distribution ","date":"2021-03-05","objectID":"/keyvalue-workloads/:3:1","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":"Key Value Sizes Memcached workloads at Twitter link Average get ratio is close to 90% indicating most of the caches are serving read-heavy workloads. More than 35% of all Twemcache clusters are write-heavy, and more than 20% have a write ratio higher than 50%. Majority of the cache workloads still follow Zipfian distribution. KV sizes are small. Note: Couldn’t find DeleteProportion in Core-properties for YCSB… ","date":"2021-03-05","objectID":"/keyvalue-workloads/:3:2","tags":["keyvalue","workload"],"title":"Key Value Workloads","uri":"/keyvalue-workloads/"},{"categories":null,"content":" LSM-tree survey WiscKey: key/value separation HashKV: store cold keys separately. LSM-trie PebblesDB KVell Skip-Tree: put kv items directly to non-adjacent layers (skipping some intermediate layers) to reduce write amplification from level-to-level data compaction. TRIAD: separate hot keys from cold keys. hot keys are stored in memtable for as long as possible. Use multiple memtables and flush them at once to disk. Pipelined Compaction Procedure: A compaction contains three phases: read phase, merge-sort phase, and write phase. Pipeline these three phases, to overlap CPU and disk IO. cLSM: LSM-tree for multi-core processors. It exploits multiprocessor-friendly data structures and non-blocking synchronization. cLSM supports a rich API, including consistent snapshot scans and general non-blocking read-modify-write operations. Towards Accurate and Fast Evaluation of Multi-Stage Log-Structured Designs Dostoevsky: evaluated against well-tuned LSM-trees. Extends the design space of LSM-trees with a new merge policy by combining leveling and tiering. very useful for certain workloads that require efficient writes, point lookups, and long range queries with less emphasis on short range queries. Mutant: decide which type of cloud disks to place SStables, based on access pattern. diff-index: use LSM-tree as secondary indexes. ","date":"2021-03-01","objectID":"/keyvalue/:0:0","tags":["keyvalue","paper"],"title":"Key Value Systems","uri":"/keyvalue/"},{"categories":null,"content":"List of ideas to explore. Interaction between a filesystem and a computational SSD that does transparent compression. Problem: How to manage SSD space in this case? What API should be use? Can use SSDSim to explore the design space. Key prefix compression in k/v store Problem: in levelDB, it uses fixed-size key prefix compression. Can we do better than this? In Rockset/Foundationdb, secondary indexes are stored as \u003ckey, null\u003e key/value pairs. This creates an opportunity that significant storage saving can be achieved if we can do a better key compression. Zoned namespace SSDs VectorBtree, VectorBloomFilter, VectorKV, VectorDB, VectorFS Investigate how to levarage AVX extension instructions to accelerate a B/B+ tree index, certain operations in a KV store, a DB or a FS. Execution plan: Understand how lookup works in an indexing data structure; determine whether we can easily do batch lookups; find an open-source implementation; modify and add support for batch lookup; measure the batch lookup speedup compared with point lookup -\u003e best result. Integrate batch lookup into some systems, such as a KV store or a DB. VectorZip Use AVX instruction to accelerate compression (zstd/lz4) ","date":"2021-02-22","objectID":"/idea-repository/:0:0","tags":["idea"],"title":"Idea Repository","uri":"/idea-repository/"},{"categories":null,"content":"The ``C++ Primer’’ book has a very interesting recommendation to use postfix operators, only when necessary (on page 148 for the 5th version). The authors shared the following in their book. The prefix version avoids unnecessary work. It increments the value and returns the incremented version. The postfix operator must store the original value so that it can return the unincremented value as its result.\" For ints and pointers, the compiler can optimize away this extra work. After reading this, I started to use prefix instead of postfix more often. However, I have always been doubting the statement the authors made in their book, until this moment. To finally test the authors’ claim, I created the following program. #include \u003ciostream\u003e int main() { long i=0, j; j = i++; j = ++i; return 0; } Here is the assembly code for the above short code. _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp xorl %eax, %eax movl $0, -4(%rbp) movq $0, -16(%rbp) // assign 0 to i // j = i++; movq -16(%rbp), %rcx // move i from memory to register rcx movq %rcx, %rdx // copy rcx to rdx addq $1, %rdx // increment rdx movq %rdx, -16(%rbp) // store rdx into i movq %rcx, -24(%rbp) // store rcx into j // j = ++i; movq -16(%rbp), %rcx // move i from memory to rcx addq $1, %rcx // increment rcx movq %rcx, -16(%rbp) // move rcx into i movq %rcx, -24(%rbp) // move rcx into j popq %rbp retq .cfi_endproc For postfix operators, the compiler indeed stores another copy in rcx and then assign it to j. We use one more register for postfix operators. // program 2 #include \u003ciostream\u003e int main() { long i=0, j; i++; ++i; return 0; } _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp xorl %eax, %eax movl $0, -4(%rbp) movq $0, -16(%rbp) // assign 0 to i // i++; movq -16(%rbp), %rcx // move i from memory to rcx addq $1, %rcx // increment rcx by 1 movq %rcx, -16(%rbp) // store rcx back to i // ++i; movq -16(%rbp), %rcx addq $1, %rcx movq %rcx, -16(%rbp) popq %rbp retq .cfi_endproc If we just increment a variable by itself, it seems the generated code would be the same for both postfix and prefix operators. ","date":"2021-02-05","objectID":"/prefix-vs-postfix/:0:0","tags":["coding","C/C++"],"title":"Use Prefix Operators (++i) vs. Postfix Operators (i++)","uri":"/prefix-vs-postfix/"},{"categories":null,"content":"The following content is selected from the Database Management Systems book, chapter 18. ARIES Three main principles lie behind the ARIES recovery algorithm: Write-Ahead Logging: Any change to a database object is first recorded in the log; the record in the log must be written to stable storage before the change to the database object is written to disk. Repeating History During Redo: On restart following a crash, ARIES retraces all actions of the DBMS before the crash and brings the system back to the exact state that it was in at the time of the crash. Then, it undoes the actions of transactions still active at the time of the crash. (effectively aborting them). Logging Changes During Undo: Changes mada to the database while undoing a transaction are logged to ensure such an action is not repeated in the event of repeated (failures causing) restarts. Every log record is given a unique id called the log sequence number (LSN). For recovery purposes, every page in the database contains the LSN of the most recent log record that describes a change to this page. This LSN is called the pageLSN. Every log record has certain fields: prevLSN, transID, and type. The set of all log records for a given transaction is maintained as a linked list going back in time, using the prevLSN field; this list must be updated whenever a log record is added. The transID field is the id of the transaction generating the log record, and the type field obviously indicates the type of the log record. Update log record format struct CommonHeader { long prevLSN; long transID; byte type; }; struct UpdateRec { CommonHeader head; long pageID int length int offset void* before-image; void* after-image; }; The before-image is the value of the changed bytes before the change; the after-image is the value after the change. A redo-only update log record contains just the after-image; similarly an undo-only update record contains just the before-image. Actions need to be logged Updating a Page: After modifying the page, an update type record (described later in this section) is appended to the log tail. The pageLSN of the page is then set to the LSN of the update log record. (The page must be pinned in the buffer pool while these actions are carried out.) Commit: when a transaction decides to commit, it force-writes a commit type log record containing the transaction id. That is, the log record is appended to the log, and the log tail is written to stable storage, up to and including the commit record. The transaction is considered to have committed at the instant that its commit log record is written to stable storage. (Some additional steps must be taken, e.g., removing the transaction’s entry in the transaction table; these follow the writing of the commit log record.) Abort: When a transaction is aborted, an abort type log record containing the transaction id is appended to the log, and Undo is initiated for this transaction (Section 18.6.3). End: As noted above, when a transaction is aborted or committed, some additional actions must be taken beyond writing the abort or commit log record. After all these additional steps are completed, an end type log record containing the transaction id is appended to the log. Undoing an update: When a transaction is rolled back (because the transaction is aborted, or during recovery frorn a crash), its updates are undone. When the action described by an update log record is undone, a compensation log record, or CLR, is written. In addition to the log, the following two tables contain important recovery-related infornlation: Transaction Table: This table contains one entry for each active transaction. The entry contains (among other things) the transaction id, the status, and a field called lastLSN, which is the LSN of the most recent log record for this transaction. The status of a transaction can be that it is in progress, corunlitted, or aborted. (In the latter two cases, the transaction will be removed from the table on","date":"2021-01-23","objectID":"/crash-recovery/:0:0","tags":["note","transaction","crash recovery"],"title":"Crash Recovery","uri":"/crash-recovery/"},{"categories":null,"content":"Write-ahead log procotol Before writing a page to disk, every update log record that describes a change to this page must be forced to stable storage. This is accomplished by forcing all log records up to and including the one with LSN equal to the pageLSN to stable storage before writing the page to disk. The definition of a committed transaction is effectively ‘a transaction all of whose log records including a commit record have been written to stable storage’. ","date":"2021-01-23","objectID":"/crash-recovery/:1:0","tags":["note","transaction","crash recovery"],"title":"Crash Recovery","uri":"/crash-recovery/"},{"categories":null,"content":"Key-value stores such as RocksDB have been used as the storage engine in several databases (MySQL/Mongo/Rockset/FoundationDB). A database typically requires support of secondary indexes. To support secondary index, a common approach is to store the secondary index as yet another key-value pairs. Let’s take a look at the following example. Employee table userid (primary key) salary age Alice 40000 30 Bob 20000 23 Charlie 30000 30 David 20000 25 To store the user table, we can store each record using tableName,primary_key -\u003e salary,age as key/value pairs. key value Employee,Alice 40000,30 Employee,Bob 20000,23 Employee,Charlie 30000,30 Employee,David 20000,25 To support secondary index based on the salary column, one could store salary,primary_key as the key with an empty value into the key/value store (or set the value to be something that would be interesting for the use case). This will ensure efficient search for employees with a particular amount of salary, as the key/value pairs are sorted by keys. key value 20000,Alice 20000,David 30000,Charlie 40000,Alice For both cases, the key/value pairs are sorted based on keys. Because keys are sorted, similar keys will be grouped together. Thus, prefix compression is important to reduce space consumption when storing keys, as done in LevelDB (in LevelDB, for every N key-value pairs in an SSTable, the key for the first key-value pair is stored in complete while the keys for the rest key-value pairs are prefix-compressed based on the key before it. Each time a complete key is stored, it is called a restart point in LevelDB. Restart points enable quick binary search within an SSTable.). Since keys are sorted and nearby keys are similar, could we use a data structure such as trie to organize data within an SSTables? Maybe make sense when values are small. Reference LSM-trie ","date":"2021-01-17","objectID":"/secondary-index-kv-store/:0:0","tags":["idea","note","key-value"],"title":"Secondary Index Built on top of a Key-value Store","uri":"/secondary-index-kv-store/"},{"categories":null,"content":"There are a few ideas/techniques that were invented a long time ago but they are being used/re-implemented again and again, even in today’s sytems. Why do people look and reimplement these techniques, even after 50 years? Because, even after 50 years, we face largely the same problem when we design new systems. Here are a few examples. Ideas First-time Introduced What problem is solved? Trasactions Make it easy to develop on top of a data store. Modifications are atomic and failures are handled automatically by the system. Log-structure storage/FS Turn random writes into sequential ones (good for hard disk drives, original motivation for developing LFS). No in-place overwrites (required for solid-state drives.). Paxos Reach concensus across a number of programs/nodes ","date":"2021-01-15","objectID":"/enduring-ideas/:0:0","tags":["techniques"],"title":"Enduring Ideas/Techniques that are still in use today","uri":"/enduring-ideas/"},{"categories":null,"content":"When I searched for how leveldb/rocksdb and a few other KV stores handle delete requests, the document will usually say ``we insert a special a tombstone value or a tombstone entry.’’. However, for KV stores which allow storing arbitrary keys and values, I am not sure which value we should use as the special tombstone value, as whatever value we pick as a tombstone value, it could be a valid value stored by the user. I looked up leveldb source code and I think I have figured out how leveldb handles this case. In stead of using a special tombstone value, it instead inserts a special tombstone record which indicates whether this is a new PUT or a delete. The internal key used in leveldb includes both an increasing sequence number but also a type field. The type field can have two values: kTypeValue (0x01) or kTypeDelete (0x00). If it is a kTypeDelete, the value field is empty and this request is a delete operation. internal_key = user_key + sequence_num (56-bit) + type (8-bit) So, essentially, for each KV pair, from a PUT or a Delete request, besides user_key/user_value, leveldb also stores a sequence number and a type which indicates whether this is a PUT or a delete (this KV is deleted). //db/memtable.h: // Add an entry into memtable that maps key to value at the // specified sequence number and with the specified type. // Typically value will be empty if type==kTypeDeletion. void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value); ","date":"2021-01-10","objectID":"/leveldb/:0:0","tags":["leveldb"],"title":"Delete Request in LevelDB","uri":"/leveldb/"},{"categories":null,"content":"NOTE: OpenGauss may fix this problem. Three special values are reserved as special transaction IDs, including invalid, bootstrap and frozen transactions. The rest values are used in a circular fashion. Once a row is older enough (controlled by vacuum_freeze_min_age, an integer value measuring the number of transactions), the vaccum process can mark its TID as FrozenTransactionId. FrozenTransactionId is smaller than any normal transaction ID and thus these frozen rows are visible for all current and future transactions. NOTE: The requirement of a periodical vaccum process to set rows as frozen (set TID to FrozenTransactionId), in order to handle TID wraparound problem does not seem optimal/elegant. Is there any better way to handle it? /* ---------------- * Special transaction ID values * * BootstrapTransactionId is the XID for \"bootstrap\" operations, and * FrozenTransactionId is used for very old tuples. Both should * always be considered valid. * * FirstNormalTransactionId is the first \"normal\" transaction id. * Note: if you need to change it, you must change pg_class.h as well. * ---------------- */ #define InvalidTransactionId ((TransactionId) 0) #define BootstrapTransactionId ((TransactionId) 1) #define FrozenTransactionId ((TransactionId) 2) #define FirstNormalTransactionId ((TransactionId) 3) #define MaxTransactionId ((TransactionId) 0xFFFFFFFF) #define TransactionIdIsValid(xid) ((xid) != InvalidTransactionId) #define TransactionIdIsNormal(xid) ((xid) \u003e= FirstNormalTransactionId) /* advance a transaction ID variable, handling wraparound correctly */ #define TransactionIdAdvance(dest) \\ do { \\ (dest)++; \\ if ((dest) \u003c FirstNormalTransactionId) \\ (dest) = FirstNormalTransactionId; \\ } while(0) /* back up a transaction ID variable, handling wraparound correctly */ #define TransactionIdRetreat(dest) \\ do { \\ (dest)--; \\ } while ((dest) \u003c FirstNormalTransactionId) ","date":"2020-12-17","objectID":"/postgres-tid-wraparound/:0:0","tags":["postgres","idea"],"title":"Postgres Handling Transaction ID Wrap Around","uri":"/postgres-tid-wraparound/"},{"categories":null,"content":"The four properties that a transaction in the database world has are ACID: A stands for atomicity, C for consistency, I for isolation and D for durability. The purpose of transactions is to maintain data in the face of concurrent access and system failures. Property Definition Technique to achieve that property Atomicity A transaction either succeeds or fails as a single operation. Undo log Consistency A database should start in a consistent state and end in another consistent state, after applying a transaction or transactions. Relies on the other three properties. Isolation Each transaction should be executed as if it was the only transaction that is running. Transactions are isolated and are not aware of each other. Allowing concurrent transactions. 2-phase locking or MVCC Durability Writes made to the database will survive system failures. Redo log Database Management Systems The following content was selected from chapter 17, Transactions. Strict Two-Phase Locking (Strict 2PL) Rule 1. If a transaction T wants to read (respectively, modify) an object, it first requests a shared (respectively, exclusive) lock on the object. Rule 2. All locks held by a transaction are released when the transaction is completed. If two transactions access the same object, and one wants to modify it, their actions are effectively ordered serially: all actions of one of these transactions (the one that gets the lock on the common object first) are completed before (this lock is released and) the other transaction can proceed. Two-Phase Locking (2PL) Rule 1. If a transaction T wants to read (respectively, modify) an object, it first requests a shared (respectively, exclusive) lock on the object (same as strict 2PL). Rule 2. A transaction cannot request additional locks once it releases any lock. Recovery Manager The recovery manager of a DBMS is responsible for ensuring transaction atomicity and durability. It ensures atomicity by undoing the actions of transactions that do not commit, and durability by making sure that all actions of committed transactions survive systenl crashes, (e.g., a core dump caused by a bus error) and media failures (e.g., a disk is corrupted). Lock Implementation in DBMS The lock manager maintains a lock table, which is a hash table with the data object identifier as the key. A lock table entry for an object – which can be a page, a record, and so on, depending on the DBMS – contains the following inforrnation: the number of transactions currently holding a. lock on the object (this can be more than one if the object is locked in shared mode), the nature of the lock (shared or exclusive), and a pointer to the queue of lock requests. Concurrency control in B+ Trees The higher levels of the tree only direct searches. All the ‘real’ data is in the leaf levels (in the forrnat of one of the three alternatives for data entries). For inserts, a node must be locked (in exclusive rnode, of course) only if a split can propagate up to it from the modified leaf. Searches should obtain shared locks on nodes, starting at the root and proceeding along a path to the desired leaf. The first observation suggests that a lock on a node can be released as soon as a lock on a child node is obtained, because searches never go back up the tree. A conservative locking strategy for inserts would be to obtain exclusive locks on all nodes as we go down from the root to the leaf node to be modified, because splits can propagate all the way from a leaf to the root. However, once we lock the child of a node, the lock on the node is required only in the event that a split propagates back to it. In particular, if the child of this node (on the path to the modified leaf) is not full when it is locked, any split that propagates up to the child can be resolved at the child, and does not propagate further to the current node. Therefore, when we lock a child node, we can release the lock on the parent if the child is not full. The locks held thus by an ins","date":"2020-09-27","objectID":"/transaction/:0:0","tags":["note","transaction","acid"],"title":"Transaction","uri":"/transaction/"},{"categories":null,"content":"Optimistic Concurrency Control * pessimistic: lock-based approach, 2PL * optimistic: do not use locks. Read-Validation-Write Optimistic concurrent control: Read: The transaction executes, reading values from the database and writing to a private workspace. Validation: If the transaction decides that it wants to commit, the DBMS checks whether the transaction could possibly have conflicted with any other concurrently executing transaction. If there is a possible conflict, the transaction is aborted; its private workspace is cleared and it is restarted. Write: If validation determines that there are no possible conflicts, the changes to data objects made by the transaction in its private workspace are copied into the database. Each transaction Ti is assigned a timstamp TS(Ti) at the beginning of its validation phase, and the validation criterion checks whether the timestamp ordering of transactions is an equivalent serial order. For every pair of transac-tions Ti and Tj such that TS(Ti) \u003c TS(Tj), one of the following validation conditions must hold: Ti completes (all three phases) before Tj begins. Ti completes before Tj starts its Write phase, and Ti does not write any database object read by Tj. Ti completes its Read phase before Tj completes its Read phase, and Ti does not write any database object that is either read or written by Tj. To validate Tj, we must check to see that one of these conditions holds with respect to each comlnitted transaction Ti such that TS(Ti) \u003c TS(Tj). Each of these conditions ensures that Tj’s modifications are not visible to Ti. Further, the first condition allows T j to see some of Ti’s changes, but clearly, they execute completely in serial order with respect to each other. The second condition allows Tj to read objects while Ti is still modifying objects, but there is no conflict because Tj does not read any object modified by Ti. Although Tj might overwrite some objects written by Ti, all of Ti’s writes precede all of Tj’s writes. The third condition allows Ti and Tj to write objects at the same time and thus have even more overlap in time than the second condition, but the sets of objects written by the two transactions cannot overlap. Thus, no RW, WR, or WW conflicts are possible if any of these three conditions is met. Checking these validation criteria requires us to maintain lists of objects read and written by each transaction. Further, while one transaction is being validated, no other transaction can be allowed to commit; otherwise, the validation of the first transaction might miss conflicts with respect to the newly committed transaction. The Write phase of a validated transaction must also be completed (so that its effects are visible outside its private workspace) before other transactions can be validated. A synchronization mechanism such as a critical section can be used to ensure that at most one transaction is in its (combined) Validation/Write phases at any time. Clearly, it is not the case that optimistic concurrency control has no overheads; rather, the locking overheads of lock-based approaches are replaced with the overheads of recording read-lists and write-lists for transactions, checking for conflicts, and copying changes from the private workspace. Similarly, the implicit cost of blocking in a lock-based approach is replaced by the implicit cost of the work wasted by restarted transactions. ","date":"2020-09-27","objectID":"/transaction/:1:0","tags":["note","transaction","acid"],"title":"Transaction","uri":"/transaction/"},{"categories":null,"content":"Improved Conflict Resolution for optimisic concurrency control problem: If Ti writes all data items required by Tj before Tj reads them, we can still commit Tj. But basic optimistic concurrency control does not permit this. We don’t know when Ti wrote the object at the time when we validate Tj, because we only have a list of read/write objects by Ti (without recording their timestamps). Details workflow is as following. Before reading a data item, a transaction enters an access entry in a hash table. The access entry contains the transaction id, a data object id, and a modified flag (initially set to false), and entries are hashed on the data object id. A temporary exclusive lock is obtained on the hash bucket containing the entry, and the lock is held while the read data item is copied from the database buffer into the private workspace of the transaction. During validation of T, the hash buckets of all data objects accessed by T are again locked (in exclusive mode) to check if T has encountered any data conflicts. T has encountered a conflict if the modified flag is set to true in one of its access entries. (This assumes that the ‘die’ policy is being used; if the ‘kill’ policy is used, T is restarted when the flag is set to true.) If T is successfully validated, we lock the hash bucket of each object modified by T, retrieve all access entries for this object (some of these entries could be added by other transactions), set the modified flag to true, and release the lock on the bucket. If the ‘kill’ policy is used, the transactions that entered these access entries are restarted. We then complete T’s Write phase. ","date":"2020-09-27","objectID":"/transaction/:2:0","tags":["note","transaction","acid"],"title":"Transaction","uri":"/transaction/"},{"categories":null,"content":"Timestamp-Based Concurrency Control This is not MVCC. Each object has a single copy. However, this protocol may permit schedules that are not recoverable. Use MVCC instead. Each transaction can be assigned a timestamp at startup, and we can ensure, at execution time, that if action a_i of transaction Ti conflicts with action a_j of transaction Tj, a_i occurs before a_j if TS(Ti) \u003c TS(Tj). If an action violates this ordering, the transaction is aborted and restarted. To implernent this concurrency control scheme, every database object O is given a read timestamp RTS(O) and a write timestamp WTS(O). If a transaction T wants to read object O, and TS(T) \u003c WTS(O), the order of this read with respect to the most recent write on O would violate the timestamp order between this transaction and the writer. Therefore, T is aborted and restarted with a new, larger timestamp. If TS(T) \u003e WTS(O), T reads O, and RTS(O) is set to the larger of RTS(O) and TS(T). (Note that a physical change–the change to RTS(O)-is written to disk and recorded in the log for recovery purposes, even on reads. This write operation is a significant overhead.) For writes, we need to check: If TS(T) \u003c RTS(O), the write action conflicts with the most recent read action of O, and T is therefore aborted and restarted. If TS(T) \u003c WTS(O), a naive approach would be to abort T because its write action conflicts with the most recent write of O and is out of timestamp order. However, we can safely ignore such writes and continue. Ignoring outdated writes is called the Thomas Write Rule. Otherwise, T writes O and WTS(O) is set to TS(T). ","date":"2020-09-27","objectID":"/transaction/:3:0","tags":["note","transaction","acid"],"title":"Transaction","uri":"/transaction/"},{"categories":null,"content":"Multiversion Concurrency Control The goal is to ensure that a transaction never has to wait to read a database object, and the idea is to maintain several versions of each database object, each with a write timestamp, and let transaction Ti read the most recent version whose timestamp precedes TS(Ti). If transaction Ti wants to write an object, we must ensure that the object has not already been read by some other transaction Tj such that TS (Ti) \u003c TS(Tj). If we allow Ti to write such an object, its change should be seen by Tj for serializability, but obviously Tj, which read the object at some time in the past, will not see Ti’s change. To check this condition, every object also has an associated read timestarnp, and whenever a transaction reads the object, the read timestamp is set to the maximum of the current read timestamp and the reader’s timestamp. If Ti wants to write an object O and TS(Ti) \u003c RTS(O), Ti is aborted and restarted with a new, larger timestamp. Otherwise, Ti creates a new version of O and sets the read and write timestamps of the new version to TS(Ti). Design Data-Intensive Applications book. Isolation is the property that provides guarantees in the concurrent accesses to data. Isolation is implemented by means of a concurrency control protocol. The simplest way is to make all readers wait until the writer is done, which is known as a read-write lock. Locks are known to create contention especially between long read transactions and update transactions. MVCC aims at solving the problem by keeping multiple copies of each data item. In this way, each user connected to the database sees a snapshot of the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed (or, in database terms: until the transaction has been committed.) 知乎link ACD三个特性是通过Redo log（重做日志）和Undo log 实现的。 而隔离性是通过锁来实现的。 重做日志用来实现事务的持久性，即D特性. Undo log (MySQL): 实现事务回滚 实现MVCC repeatable read vs read committed 在innodb中(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来; 在innodb中(默认read committed级别), 事务中每条select语句都会创建一个快照(read view); 其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除（谁会对刚插入的数据有可见性需求呢！！），而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo 在回滚段中的undo logs分为: insert undo log 和 update undo log insert undo log : 事务对insert新记录时产生的undolog, 只在事务回滚时需要, 并且在事务提交后就可以立即丢弃。(The new record is stored in the table itself. No need to serve reads from undo log. Undo records for insert only serve for transaction rollback. log only needs to tell when that new record was inserted so that we can determine when to undo it (remove it during rollback).) update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除 ","date":"2020-09-27","objectID":"/transaction/:4:0","tags":["note","transaction","acid"],"title":"Transaction","uri":"/transaction/"},{"categories":null,"content":"Hit the Road is an iOS app for tracking ourdoor exercises. It can track walking, running, and biking. It is supposed to run on iPhones, but not iPads or iWatchs. User Privacy The app only stores the duration and mileage for each activity locally in the installed device. The app does not store the GPS locations. It only uses GPS locations during an activity to get the mileage. The app does not send any data over the network. Support For bug reports and feature requests, please send emails to linxingnku@gmail.com, with a topic starting with “Hit the Road:”. ","date":"2020-08-23","objectID":"/hit-the-road/:0:0","tags":["iOS","support"],"title":"Hit the Road iOS App","uri":"/hit-the-road/"},{"categories":null,"content":"In YCSB, we can also use the options file (-p rocksdb.optionsfile=/tmp/ycsb-options.ini, e.g.,) to configure the options for RocksDB. However, for a new user, it is actually not easy to figure out what needs to be configured for RocksDB for YCSB. Here is a quick way to get it start. Use the ycsb tool to load a rocksdb without specifying the options. $ ./bin/ycsb load rocksdb -s -P workloads/workloada -p rocksdb.dir=/tmp/ycsb-rocksdb-data Then, when we look into the rocksdb datadir, we can actually be able to find the option file that is generated automatically: OPTIONS-00008 or with a different number. We can copy out this options file to somewhere else, and delete all files in the rocksdb dir. Next, we can modify the options file. Specifically, we should modify the CFOptions and TableOptions/BlockBasedTable for the usertable sections, as YCSB will use this as the database (or column family) for loading data and running tests. Hope this helps. ","date":"2020-08-08","objectID":"/ycsb-rocksdb-options/:0:0","tags":["tips","rocksdb"],"title":"Use Options File for Benchmarking RocksDB using YCSB","uri":"/ycsb-rocksdb-options/"},{"categories":null,"content":"NFS sponsored a data storage visioning workshop in 2018. The workshop was hosted by IBM Research. From the workshop, they produced a report, summarizing the discussion from the workshop. The report outlines the future research challenges and opportunities that the attendees recommend to work on. The discussion of the workshop was organized in the four groups. Below are some interesting points I copied from the report. Storage for the Cloud, Edge, and IoT Systems More desegregated, composable software architectures are highly desirable. One common feature of all IoT devices is that they have limited hardware resources. To address this constraint, we should explore how to identify and discard unimportant data in a timely fashion, and how to balance among storage, preprocessing, and communication between IoT devices and cloud. AI and Storage The increasing richness of data (e.g., multiple correlated mixed type streams, images, sound, video) requires more complex ML and Deep Learning (DL) approaches. AI stage awareness. Storage that is aware of the distinct stages or phases of AI processing can optimize AI pipelines via techniques such as caching of intermediate results, tracking of lineage, provenance, and checkpointing [46, 62, 72, 84]. Compute architecture and data optimization. AI platforms follow distinct distributed computation architecture patterns (e.g., data parallel and model parallel [57, 95]). Memory hierarchy and data layout design for such computation patterns should also be a focus for future storage research. APIs that express the data access intent of an AI algorithm can also be a powerful tool to integrate memory hierarchy and data layout optimizations with AI computation. Unique characteristics. AI algorithms have unique characteristics that can be exploited to create efficient storage designs. Example characteristics include a tolerance to small amounts of data loss, very structured access patterns, and the ability to use and extrapolate from lossy compression Access and distribution characteristics. Emerging access methods and characteristics associated with AI workloads, such as stream processing [9] or edge storage [12], also create unique challenges that should be focused on in future storage research. Security and compliance. The use of AI brings a new dimension to data security. As industries and users demand that decisions made by AI algorithms be reproducible, transparent, and explainable, pressure builds on enterprises to put in place data-management mechanisms to govern what data is and how it should be used to generate AI models and consequent insights [8, 10, 11, 35, 48]. Performance and placement optimizations. ML algorithms can be applied to predict popular data and application patterns, which help improve various storage techniques, including tiered caching, prefetching, and resource provisioning. Adapting caching policy using online learning can have significant benefits: recent work [93] shows that using ML techniques to select between LRU and LFU replacement policies resulted in a significantly improved total cache hit rate under even smaller memory constraints. The key takeaway is that ML techniques are valuable for solving online optimization problems such as caching with the caveat that the primary knobs of control have to be orthogonal. We believe that ML can be applied with great success for other problems such as non-datapath server-side caches [36, 54, 58, 60, 70, 80, 83], distributed storage caches such as in hyper-converged systems, dynamic multitiered and hybrid storage systems [50, 64, 86, 94], and hybrid systems comprising DRAM and persistent memory. Intelligent storage devices within storage systems. Computing-enabled storage devices refer to storage devices with ample internal computing capability, typically also including some AI capabilities. On one hand, computing storage devices assume part of the storage-related computing functionality so that the running storage system is a","date":"2020-07-23","objectID":"/data-storage-research-vision-2025/:0:0","tags":["research","storage","vision"],"title":"Data Storage Research Vision 2025","uri":"/data-storage-research-vision-2025/"},{"categories":null,"content":"Quite often, we need to know the network round-trip time (RTT) between two nodes and a fairly well known tool is ping. However, I found out the proper way to use the ping tool only until today. Actually, I guess I should just use the netperf tool instead, to measure the network latency in the future. By default, ping sends out a packet every second. The interval which ping uses to send out a packet can have a huge impact on the measured network latency. We can use the -i option, to set the interval. Below are the RTT values I collected between two servers connected directly with a cable. Interval Latency (us) default (1s) 161 200 ms 132 20 ms 130 2 ms 73 flood (-f) 22 As we can see, one can get very different RTTs when using different intervals. For the common cases, I guess we should use the flooding mode where a new packet is sent out immediately after the previous packet is received. netperf is another tool used for network performance testing as well. By default, it sends out a request immediately after the previous one completes. One can use netperf to test TCP/UDP round-trip time. It is probably the preferred tool to do latency measurement, than ping. netperf shows the TCP RTT is 37 us for the same two servers where I used ping to test RTT. To use netperf, we need to first start netserver at one server. Then, at the other server, start netperf to do the tests. $netperf -p 12345 -H 172.16.0.25 -l 100 -t TCP_RR -- -o min_latency,mean_latency,max_latency,stddev_latency,transaction_rate One can download the netperf source code from git. However, they have removed the configure file in the source tree. To generate the configure file, we can run the autogen.sh script. We may also need to install texinfo package, to get the makeinfo tool. Reference Measuring network latency in the cloud ","date":"2020-07-22","objectID":"/measure-network-latency/:0:0","tags":["note","tip","network latency","tech"],"title":"Measure Network Latency","uri":"/measure-network-latency/"},{"categories":null,"content":"I really liked the paper. The idea is neat. The paper is also well written and the design and the new idea is clearly presented. A very good paper to read and learn from on how to write good papers. Summary The key idea is to convert random small writes into large sequential writes at the device driver. This reduces data fragmentation and also reduces request handling overhead for the SSD, as it can combine multiple small write requests into fewer large write requests. The SSD reserves a special LBA range, called random write log buffer (RWLB), whose logical addresses can be used as temporary LBAs for small random write requests. When the device driver receives small random write requests, they are translated into large sequential write requests, using temporary LBAs from RWLB. For example, write requests to four random pages can be converted into one 4-page sequential write request into the RWLB area. The device driver then only needs to send the resulting large write request to the SSD, instead of 4 small write requests. Two new write requests are added to send data and metadata for sequentialized write requests to the SSD: twrite_data() and twrite_header(). twrite_data() sends data pages to the SSD. twrite_header() sends the orignal/actual LBA to the SSD, which will be stored in the OOB area for mapping table recovery in crash. A read redirect table is maintained in the host DRAM, to redirect read requests for sequentialized write requests, to read from the RWLB area, instead of their original LBAs. When the RWLB space is exausted, a remap operation is issued to the SSD, to restore the mapping entries for sequentialized pages to their original LBAs. No data copying is needed during this remap process. ","date":"2020-06-28","objectID":"/shrd/:0:0","tags":["paper","SSD","FTL"],"title":"SHRD: Improving Spatial Locality in Flash Storage Accesses by Sequentializing in Host and Randomizing in Device","uri":"/shrd/"},{"categories":null,"content":"paper summary for Copysets, published at USENIX ATC13","date":"2020-06-18","objectID":"/copysets/","tags":["paper","storage","replication"],"title":"Copysets: Reducing the Frequency of Data Loss in Cloud Storage","uri":"/copysets/"},{"categories":null,"content":"Summary Copysets proposed another approach/perspective for data replication, than the traditional random replication. In random replication, N nodes are selected randomly from the cluster to store a data chunk. Random replication provides a strong protection against uncorrelated failures. For each individual chunk, since it can be stored randomly at N nodes in the cluster, it is quite resilient to data loss. However, when we consider all data chunks, any N node failure will almost lead to loss of some chunks, as long as these chunks happen to be replicated at these exact N nodes. For certain large scale deployments, they prefer to have much less frequent data loss and are willing to achieve that, even at the expense of larger amount of data loss during each data loss. They introduced the concept of Copysets: a set of unique nodes that are used to store a copy of a chunk. Only when all nodes in a copyset are down will we lose that chunk. By limiting the number of copysets in a cluster, one can reduce the number of data loss probability. Terms used in the paper R: replication factor. Also means the number of replicas of each chunk. Copyset: a set of R unique nodes, to store a chunk N: the number of nodes in a cluster S: scatter width. Each node’s data is split uniformly across a group of S other nodes. P: the number of permutations used, to generate the copysets. P = S/(R-1) How it works Use permutations, P times, to generate copysets. Each copyset overlaps with another by only one node. The copysets covers all nodes equally, for load balancing. Pick a random node in the cluster as the primary node. The other replicas can be placed on the nodes of a randomly chosen copyset that contains the primary node. My Comments This paper is not easy to follow. The scatter width and the copyset are not easy to understand simultaneously. The introduction of the abbreviations makes the paper harder to understand. I forgot what P stands for when reading the paper. The paper cited from their collaborators in large paragraphs, to help motivate their work. I find this can be done in a more professional approach. A lot of simulation results. Quite surprised to see in a full ATC paper. The evaluation on HDFS/RAMCloud was quite light. ","date":"2020-06-18","objectID":"/copysets/:0:0","tags":["paper","storage","replication"],"title":"Copysets: Reducing the Frequency of Data Loss in Cloud Storage","uri":"/copysets/"},{"categories":null,"content":"How time flies! I am 33 years old now and have lived in US for more than 10 years. I have also graduated from University of Utah for almost 5 years now. Wahoo! Each week passed very quickly. Now is the time to pause a little bit and reflect what has been achieved and what could be done better. Achievements Worked on a few projects/papers and made a few submissions to FAST/ATC/HotStorage, though none of them have been accepted yet. The D3 paper gets very close to be accepted by ATC18. Mentored four interns. Really enjoyed working with some of them. Learned a few new things: AWS S3/Azure, Golang, AWS serverless, and Spark. Successful collaboration with a few faculties, include Haryadi Gunawi and Feng Yan. Submitted the greencard application, though still waiting for VISA for approval. Bought a house at Fremont, CA and moved in in February this year. Areas to improve Make a career plan and see where you want to go in the next ~5 years and work diligently toward the goal. Systematically build your knowledge about storage systems. Read more documents/source code of existing storage techniques and systems. Do more hand-on experiments, trying/implementing these techniques/algorithms. Learning on the way. ","date":"2020-06-18","objectID":"/33rd-birthday/:0:0","tags":["reflection"],"title":"On my 33rd Birthday","uri":"/33rd-birthday/"},{"categories":null,"content":"This blog lists tools/simulators/platforms people used to evaluate SSD researches. MQSim MQSIM FAST18, ATC19-Kim SSDSim SSDSim Microsoft Patch [ATC12][atc12] FEMU FEMU-fast18-short ","date":"2020-06-17","objectID":"/ssd-evaluation-how/:0:0","tags":["SSD"],"title":"How to Evaluate Research Ideas for SSDs","uri":"/ssd-evaluation-how/"},{"categories":null,"content":"Caching LRU ARC File systems EXT filesystems btrfs Data Reliability RAID Erasure coding Solid State Drive FTL K/V stores LevelDB/RocksDB Distributed Storage Systems Ceph Deduplication ","date":"2020-06-14","objectID":"/foundamentals-of-storage-system/:0:0","tags":["outline"],"title":"Foundamentals of a Storage System","uri":"/foundamentals-of-storage-system/"},{"categories":null,"content":"When writing to a FILE object in C, it turns out there are multiple steps one needs to take in order to flush data from an application buffer to the stable storage (disk). Assume we have a FILE object pointer, called fp. After the fwrite(fp) call, one first needs to call fflush(fp), to flush the application buffer to the OS kernel page cache. Since the data still sits in the OS kernel space, it won’t survive on a power lose. To flush the OS page cache to the stable storage which will survive a power lose, we need to further call fsync(fileno(fp)). fsync() will make sure to flush the data in the kernel to the storage media. # to force flushing data to the storage media after fwrite() assert( fflush(fp)==0 ); assert( fsync(fileno(fp))==0 ); Reference: Ensuring data reaches disk ","date":"2019-09-02","objectID":"/flush-data-to-disk/:0:0","tags":["note"],"title":"Flush data to stable storage in C","uri":"/flush-data-to-disk/"},{"categories":null,"content":" When substantial changes have been made to a software, review the changes once before running the code. In most cases, you will be able to identify simple bugs/errors/typos in your code by just reviewing your code once. Jump directly to run and debug your code will probably take you more time to fix the program. Learn one piece at a time. When learning a new thing, it could come with many new concepts, making it harder to understand all at once. In such cases, try to understand one concept well, then move on to the next one. This reduces what needs to be understood at one time and makes it easier to learn. 分解敌军，各个击破。 Indecision is a decision to not reach your potential! Make decisions for yourself! Don’t let time fly quickly! Be responsible for your own life and own it. Do what you would like to do. 磨刀不误砍柴工。欲速则不达。 What to learn is different from what could be published. What to learn covers a much broader scope, including these that have been published before. One should continue to learn/try something, even if it does not necessarily lead to potential publications. Without learning what are there already, it is hard to know the state-of-the-art and may re-invent the wheel. ","date":"2019-08-26","objectID":"/self-notes/:0:0","tags":["note"],"title":"Notes for Myself","uri":"/self-notes/"},{"categories":null,"content":" Compile kernel with CONFIG_DYNAMIC_DEBUG=y Mount debugfs, if not mounted yet sudo mount -t debugfs none /sys/kernel/debug Turn on debug for a specific module echo 'module pblk +pfl' \u003e /sys/kernel/debug/dynamic_debug/control The flags are: p enables the pr_debug() callsite. f Include the function name in the printed message l Include line number in the printed message m Include module name in the printed message t Include thread ID in messages not generated from interrupt context _ No flags are set. (Or'd with others on input) ","date":"2019-08-13","objectID":"/kernel-dynamic-debug/:0:0","tags":["kernel","note"],"title":"Dynamic Kernel Debugging","uri":"/kernel-dynamic-debug/"},{"categories":null,"content":" In Linux pblk implementation, a line corresponds to a chunk which is one erase block. A LUN is a PU. A group is a channel. Eventually, pblk calls pblk_submit_read() and pblk_submit_write() for reads/writes to the device. Read code path (from the media) in pblk /* pblk-read.c */ pblk_submit_read() pblk_read_ppalist_rq(); or pblk_read_rq(); pblk_lookup_l2p_seq(); // get physical addresses pblk_submit_io(pblk, rqd); nvm_submit_io(dev, rqd); dev-\u003eops-\u003esubmit_io(dev, rqd); Write code path in pblk pblk_write_to_cache() { for (i = 0; i \u003c nr_entries; i++) { void *data = bio_data(bio); w_ctx.lba = lba + i; pos = pblk_rb_wrap_pos(\u0026pblk-\u003erwb, bpos + i); pblk_rb_write_entry_user(\u0026pblk-\u003erwb, data, w_ctx, pos); bio_advance(bio, PBLK_EXPOSED_PAGE_SIZE); } pblk_write_should_kick(struct pblk *pblk); } /* pblk-rb.c */ void pblk_rb_write_entry_user(struct pblk_rb *rb, void *data, struct pblk_w_ctx w_ctx, unsigned int ring_pos) { __pblk_rb_write_entry(rb, data, w_ctx, entry); pblk_update_map_cache(pblk, w_ctx.lba, entry-\u003ecacheline); } /* pblk-core.c */ void pblk_write_should_kick(struct pblk *pblk) { unsigned int secs_avail = pblk_rb_read_count(\u0026pblk-\u003erwb); if (secs_avail \u003e= pblk-\u003emin_write_pgs_data) pblk_write_kick(pblk); } pblk_write_kick() wake_up_process(pblk-\u003ewriter_ts); pblk_write_ts() /* pblk-write.c */ pblk_submit_write(pblk, $secs_left); pblk_submit_write(struct pblk* pblk, int *secs_left) pblk_rb_read_to_bio(\u0026pblk-\u003erwb, rqd, pos, secs_to_sync, secs_avail) pblk_submit_io_set(pblk, rqd); pblk_setup_w_rq(pblk, rqd, \u0026erase_ppa); // will call pblk_map_rq() to allocate new physical pages; will also call pblk_line_erase() to first erase if needed. pblk_submit_io(pblk, rqd); nvm_submit_io(dev, rqd); dev-\u003eops-\u003esubmit_io(dev, rqd); Garbage Collection code path in pblk Erase operations are done during writes (when flushing the write buffer to the media). /* pblk-gc.c */ int pblk_gc_init(struct pblk *pblk) { gc-\u003egc_ts = kthread_create(pblk_gc_ts, pblk, \"pblk-gc-ts\"); gc-\u003egc_writer_ts = kthread_create(pblk_gc_writer_ts, pblk, \"pblk-gc-writer-ts\"); gc-\u003egc_reader_ts = kthread_create(pblk_gc_reader_ts, pblk, \"pblk-gc-reader-ts\"); /* Workqueue that reads valid sectors from a line and submit them to the * GC writer to be recycled. */ gc-\u003egc_line_reader_wq = alloc_workqueue(\"pblk-gc-line-reader-wq\", WQ_MEM_RECLAIM | WQ_UNBOUND, PBLK_GC_MAX_READERS); /* Workqueue that prepare lines for GC */ gc-\u003egc_reader_wq = alloc_workqueue(\"pblk-gc-line_wq\", WQ_MEM_RECLAIM | WQ_UNBOUND, 1); } pblk_gc_ts() pblk_gc_run() pblk_gc_get_victim_line() pblk_gc_reader_kick() pblk_gc_reader_ts() pblk_gc_read() pblk_gc_line() pblk_gc_line_prepare_ws() pblk_gc_line_ws(); pblk_submit_read_gc(pblk, gc_rq); read_ppalist_rq_gc(); // find all nonempty pages. pblk_submit_io_sync(); list_add_tail(\u0026gc_rq-\u003elist, \u0026gc-\u003ew_list); // add nonempty pages to the gc-\u003ew_list pblk_gc_writer_kick(\u0026pblk-\u003egc); // kick writer to do writes kref_put(\u0026line-\u003eref, pblk_line_put); // add this line back to free list, if there is no reference. pblk_gc_writer_ts() pblk_gc_write(pblk) pblk_write_gc_to_cache(pblk, gc_rq); pblk_rb_write_entry_gc(); __pblk_rb_write_entry(); pblk_update_map_gc(); Scratch space /* pblk-init.c */ pblk_make_rq() if (bio_op(bio) == REQ_OP_DISCARD) pblk_discard(pblk, bio); if (bio_data_dir(bio) == READ) { blk_queue_split(q, \u0026bio); pblk_submit_read(pblk, bio); } else { if (pblk_get_secs(bio) \u003e pblk_rl_max_io(\u0026pblk-\u003erl)) blk_queue_split(q, \u0026bio); pblk_write_to_cache(pblk, bio, PBLK_IOTYPE_USER); } /* get phyiscal address fro logical address */ ppa = pblk_trans_map_get(pblk, lba) int pblk_lookup_l2p_seq(struct pblk *pblk, struct ppa_addr *ppas, sector_t blba, int nr_secs, bool *from_cache) void pblk_lookup_l2p_rand(struct pblk *pblk, struct ppa_addr *ppas, u64 *lba_list, int nr_secs) /* pblk-read.c */ pblk_submit_read() pblk_read_ppalist_rq(); or pblk_read_rq(); pblk_lookup_l2p_seq(); // get physical addresses pblk_submit_io(pblk, rqd); nvm_submit_io(dev, rqd); dev-\u003eops-\u003esubmit_io(dev, rqd); ","date":"2019-08-13","objectID":"/ocssd/:0:0","tags":["engineering","open channel","SSD"],"title":"Open-Channel SSD","uri":"/ocssd/"},{"categories":null,"content":"So you have your paper accepted to a conference and completed your camera ready version. Now is the time to create an author version for it so that you can post it on your own web site! The trick is actually quite simple if you are using the ACM Latex template. In acmart.cls, change the execution option for authorversion from false to true. -\\ExecuteOptionsX{authorversion=false} +\\ExecuteOptionsX{authorversion=true} You may also want to add page numbers into your paper. \\pagestyle{plain} \\pagenumbering{arabic} ","date":"2019-04-28","objectID":"/authorversion/:0:0","tags":["paper"],"title":"Create The Author Version for Your Paper (ACM)","uri":"/authorversion/"},{"categories":null,"content":"This is an introductory book about programming, written by Bjarne Stroustrup, the C++ creator. ","date":"2019-04-25","objectID":"/programming/:0:0","tags":["book"],"title":"Programming - Principles and Practice Using C++","uri":"/programming/"},{"categories":null,"content":"Quotes Programming is (among other things) a practical skill that you need to practice to master. Learning involves repetition. You must run before you can walk! Babies really do run by themselves before they learn the finer skills of slow, controlled walking. Similarly, you will dash ahead, occasionally stumbling, to get a feel of programming before slowing down to gain the necessary finer control and understanding. ","date":"2019-04-25","objectID":"/programming/:1:0","tags":["book"],"title":"Programming - Principles and Practice Using C++","uri":"/programming/"},{"categories":null,"content":"A paper about garbage collection for SSD Arrays","date":"2019-03-08","objectID":"/global-garbage-collection/","tags":["SSD","Garbage Collection","paper"],"title":"Coordinating Garbage Collection for Arrays of Solid-State Drives","uri":"/global-garbage-collection/"},{"categories":null,"content":"This is the start of a series of blogs I plan to write about existing work related with garbage collection for SSDs. The main idea of this paper is to run garbage collection during idle times to minimize the impact on foreground workloads. Garbage collection is scheduled to run simultaneously at all SSDs, to maximize the time window during which there is no garbage collection, and thus higher application performance. Figure 5 shows their approach and Figure 10 shows the effects. I do have a few reservations about this paper. Most of the evaluations focus on workloads with 40% or more write requests. If a workload has 40% or more write requests, I wonder how many of the write requests will be sequential and how many are random. If a workload’s write requests are mostly random, write requests should not be as high as 40%. On the other hand, if a workload contains 40% or more write requests, I would assume most of these write requests will be sequential. Sequential writes however are much easier to handle. The paper assumed it is a solved problem to predict idle periods. But what if the system is busy or has some small work all the time? The paper only looked at average response time. They should look at tail latencies as well. Their approach might experience extremely long tail latencies, because they schedule to do garbage collection simultaneously at all SSDs. We note a 60% reduction in response time for the HPC(R) read-dominated load and a 71% reduction for the HPC(W) write-dominated load. 71% reduction for HPC(W) is fine but I don’t understand why HPC(R) also received 60% reduction. HPC(R) is read intensive and for read intensive workloads, Figure 1 in their paper show SSDs are able to provide consistent high performance. Besides, read intensive workloads should not trigger lots of garbage collection and thus the reduction should be much smaller. Interestingly the baseline requires eight SSDs to provide a response time equivalent to that delivered by two devices using GGC. Even with 18 devices, the baseline performs 184% worse than GGC using only 4 devices. Such results probably mean something was not right… It is mainly because (i) HPC workloads have much higher I/O demands than Enterprise workloads (refer to Table 8), and (ii) large requests in the HPC workloads are more frequently conflict with GC invocation of drives, increasing the I/O response times. The first reason is simply flawed. Openmail has a 2x higher request rate and TPC-C’s request rate is almost as high as HPC workloads. ","date":"2019-03-08","objectID":"/global-garbage-collection/:0:0","tags":["SSD","Garbage Collection","paper"],"title":"Coordinating Garbage Collection for Arrays of Solid-State Drives","uri":"/global-garbage-collection/"},{"categories":null,"content":"DAX in ext2 filesystem","date":"2019-01-24","objectID":"/ext2-dax/","tags":["engineering","linux","source code","ext2","dax"],"title":"DAX in ext2 filesystem","uri":"/ext2-dax/"},{"categories":null,"content":"msync syscall /mm/msync.c / * MS_SYNC syncs the entire file - including mappings. */ SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags) vfs_fsync_range(file, fstart, fend, 1); file-\u003ef_op-\u003efsync(file, start, end, datasync); == ext2_fsync(); generic_file_fsync(file, start, end, datasync); __generic_file_fsync(struct file *file, loff_t start, loff_t end, int datasync) file_write_and_wait_range(file, start, end); __filemap_fdatawrite_range(mapping, lstart, lend, WB_SYNC_ALL); do_writepages(mapping, \u0026wbc); mapping-\u003ea_ops-\u003ewritepages(mapping, wbc); ext2_dax_writepages() dax_writeback_mapping_range(mapping, mapping-\u003ehost-\u003ei_sb-\u003es_bdev, wbc); dax_writeback_one(\u0026xas, dax_dev, mapping, entry); dax_flush(dax_dev, page_address(pfn_to_page(pfn)), size); arch_wb_cache_pmem(addr, size); clean_cache_range(addr, size); for (p = (void *)((unsigned long)addr \u0026 ~clflush_mask); p \u003c vend; p += x86_clflush_size) clwb(p); static const struct address_space_operations ext2_dax_aops = { .writepages = ext2_dax_writepages, .direct_IO = noop_direct_IO, .set_page_dirty = noop_set_page_dirty, .invalidatepage = noop_invalidatepage, }; void ext2_set_file_ops(struct inode *inode) { inode-\u003ei_op = \u0026ext2_file_inode_operations; inode-\u003ei_fop = \u0026ext2_file_operations; if (IS_DAX(inode)) inode-\u003ei_mapping-\u003ea_ops = \u0026ext2_dax_aops; else if (test_opt(inode-\u003ei_sb, NOBH)) inode-\u003ei_mapping-\u003ea_ops = \u0026ext2_nobh_aops; else inode-\u003ei_mapping-\u003ea_ops = \u0026ext2_aops; } ","date":"2019-01-24","objectID":"/ext2-dax/:0:1","tags":["engineering","linux","source code","ext2","dax"],"title":"DAX in ext2 filesystem","uri":"/ext2-dax/"},{"categories":null,"content":"nvdimm driver /drivers/nvdimm/pmem.h /drivers/nvdimm/pmem.c pmem_dax_direct_access() __pmem_direct_access() pmem_copy_from_iter() copy_from_iter_flushcache(addr, bytes, i); _copy_from_iter_flushcache(addr, bytes, i); iterate_and_advance(i, bytes, v, __copy_from_user_flushcache((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len), memcpy_page_flushcache((to += v.bv_len) - v.bv_len, v.bv_page, v.bv_offset, v.bv_len), memcpy_flushcache((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len) ) pmem_rw_page(struct block_device *bdev, sector_t sector, struct page *page, unsigned int op); pmem_do_bvec(pmem, page, hpage_nr_pages(page) * PAGE_SIZE, 0, op, sector); if (!op_is_write(op)) { read_pmem(page, off, pmem_addr, len); flush_dcache_page(page); } else { flush_dcache_page(page); write_pmem(pmem_addr, page, off, len); } write_pmem() memcpy_flushcache() __memcpy_flushcache(dst, src, cnt); /* assembly code ... */ while (size \u003e= 32) { asm(\"movq (%0), %%r8\\n\" \"movq 8(%0), %%r9\\n\" \"movq 16(%0), %%r10\\n\" \"movq 24(%0), %%r11\\n\" \"movnti %%r8, (%1)\\n\" \"movnti %%r9, 8(%1)\\n\" ... :: \"r\" (source), \"r\" (dest) : \"memory\", \"r8\", \"r9\", \"r10\", \"r11\"); dest += 32; source += 32; size -= 32; } static void write_pmem(void *pmem_addr, struct page *page, unsigned int off, unsigned int len) { unsigned int chunk; void *mem; while (len) { mem = kmap_atomic(page); chunk = min_t(unsigned int, len, PAGE_SIZE); memcpy_flushcache(pmem_addr, mem + off, chunk); kunmap_atomic(mem); len -= chunk; off = 0; page++; pmem_addr += PAGE_SIZE; } } static blk_status_t read_pmem(struct page *page, unsigned int off, void *pmem_addr, unsigned int len) { unsigned int chunk; unsigned long rem; void *mem; while (len) { mem = kmap_atomic(page); chunk = min_t(unsigned int, len, PAGE_SIZE); rem = memcpy_mcsafe(mem + off, pmem_addr, chunk); kunmap_atomic(mem); if (rem) return BLK_STS_IOERR; len -= chunk; off = 0; page++; pmem_addr += PAGE_SIZE; } return BLK_STS_OK; } static const struct block_device_operations pmem_fops = { .owner = THIS_MODULE, .rw_page = pmem_rw_page, .revalidate_disk = nvdimm_revalidate_disk, }; static const struct dax_operations pmem_dax_ops = { .direct_access = pmem_dax_direct_access, .copy_from_iter = pmem_copy_from_iter, .copy_to_iter = pmem_copy_to_iter, }; ","date":"2019-01-24","objectID":"/ext2-dax/:0:2","tags":["engineering","linux","source code","ext2","dax"],"title":"DAX in ext2 filesystem","uri":"/ext2-dax/"},{"categories":null,"content":"ext2 # /fs/ext2/file.c ext2_file_read_iter() ext2_dax_read_iter() dax_iomap_rw(); iomap_apply(); dax_iomap_actor(); dax_direct_access(); dax_copy_from_iter(); dax_copy_to_iter(); dax_dev-\u003eops-\u003ecopy_from_iter(dax_dev, pgoff, addr, bytes, i); static const struct vm_operations_struct ext2_dax_vm_ops = { .fault = ext2_dax_fault, /* * .huge_fault is not supported for DAX because allocation in ext2 * cannot be reliably aligned to huge page sizes and so pmd faults * will always fail and fail back to regular faults. */ .page_mkwrite = ext2_dax_fault, .pfn_mkwrite = ext2_dax_fault, }; static int ext2_file_mmap(struct file *file, struct vm_area_struct *vma) { if (!IS_DAX(file_inode(file))) return generic_file_mmap(file, vma); file_accessed(file); vma-\u003evm_ops = \u0026ext2_dax_vm_ops; return 0; } ","date":"2019-01-24","objectID":"/ext2-dax/:0:3","tags":["engineering","linux","source code","ext2","dax"],"title":"DAX in ext2 filesystem","uri":"/ext2-dax/"},{"categories":null,"content":"Where do ideas come from? Ideas come from two sources: talking to real users with real problems and then trying to solve them. This ensures somebody cares about the ideas and the rubber meets the road and not the sky. The second source is to bounce possibly good (or bad) ideas off colleages that will challenge them. In summary, the best chance for generating a good idea is to spend time in the real world and find an enviroment where you will be intellectually challenged. ","date":"2019-01-17","objectID":"/making-databases-work/:1:0","tags":["book"],"title":"Wisdom from Michael Stonebraker","uri":"/making-databases-work/"},{"categories":null,"content":"Contributions by Systems ingres: together with system R from IBM, they demonstrated that it is possible to build practical relational systems based on Todd’s relational model. ","date":"2019-01-17","objectID":"/making-databases-work/:2:0","tags":["book"],"title":"Wisdom from Michael Stonebraker","uri":"/making-databases-work/"},{"categories":null,"content":"Quotes Do something important and set it free. ","date":"2019-01-17","objectID":"/making-databases-work/:3:0","tags":["book"],"title":"Wisdom from Michael Stonebraker","uri":"/making-databases-work/"},{"categories":null,"content":"Influential papers The design and implementation of ingres. The implementation of postgres ","date":"2019-01-17","objectID":"/making-databases-work/:4:0","tags":["book"],"title":"Wisdom from Michael Stonebraker","uri":"/making-databases-work/"},{"categories":null,"content":"Read path in the Linux kernel","date":"2019-01-16","objectID":"/linux-read-path/","tags":["engineering","linux","source code"],"title":"Read syscall implementation in Linux Kernel","uri":"/linux-read-path/"},{"categories":null,"content":"Interactions among vfs, page cache, and ext2 on serving a read request stack trace for ext2_readpages() in ext2 [ 84.037457] Call Trace: [ 84.037458] dump_stack+0x46/0x5b [ 84.037460] ext2_readpages+0x3e/0x90 [ 84.037464] read_pages+0x71/0x1a0 [ 84.037470] ? __do_page_cache_readahead+0x1c9/0x1e0 [ 84.037472] __do_page_cache_readahead+0x1c9/0x1e0 [ 84.037474] ondemand_readahead+0x171/0x2b0 [ 84.037478] ? pagecache_get_page+0x30/0x2c0 [ 84.037481] ? __kernel_text_address+0xe/0x30 [ 84.037483] generic_file_read_iter+0x875/0xda0 [ 84.037412] ext2_file_read_iter+0x4c/0xe0 [ 84.037488] new_sync_read+0x12e/0x1d0 [ 84.037490] vfs_read+0x91/0x130 [ 84.037492] ksys_read+0x52/0xc0 [ 84.037494] do_syscall_64+0x4f/0x100 [ 84.037496] entry_SYSCALL_64_after_hwframe+0x44/0xa9 [ 84.037379] Call Trace: [ 84.037405] dump_stack+0x46/0x5b [ 84.037412] ext2_file_read_iter+0x4c/0xe0 [ 84.037419] new_sync_read+0x12e/0x1d0 [ 84.037426] vfs_read+0x91/0x130 [ 84.037428] ksys_read+0x52/0xc0 [ 84.037431] do_syscall_64+0x4f/0x100 [ 84.037438] entry_SYSCALL_64_after_hwframe+0x44/0xa9 read/write syscall definitions read/write syscalls are defined in fs/read_write.c, as following. SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count) { return ksys_read(fd, buf, count); } SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count) { return ksys_write(fd, buf, count); } Read syscall call stack ksys_read(fd, buf, count); vfs_read(f.file, buf, count, \u0026pos); __vfs_read(file, buf, count, pos); if (file-\u003ef_op-\u003eread) return file-\u003ef_op-\u003eread(file, buf, count, pos); else if (file-\u003ef_op-\u003eread_iter) return new_sync_read(file, buf, count, pos); else return -EINVAL; /* Take ext2 as an example. ext2 does not define read() but defines read_iter(). */ const struct file_operations ext2_file_operations = { .llseek = generic_file_llseek, .read_iter = ext2_file_read_iter, .write_iter = ext2_file_write_iter, .unlocked_ioctl = ext2_ioctl, #ifdef CONFIG_COMPAT .compat_ioctl = ext2_compat_ioctl, #endif .mmap = ext2_file_mmap, .open = dquot_file_open, .release = ext2_release_file, .fsync = ext2_fsync, .get_unmapped_area = thp_get_unmapped_area, .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write, }; new_sync_read() call_read_iter(filp, \u0026kiocb, \u0026iter); file-\u003ef_op-\u003eread_iter(kio, iter); ext2_file_read_iter() ext2_file_read_iter(): ext2_file_operations.read_iter generic_file_read_iter() generic_file_buffered_read() find_get_page() page_cache_sync_readahead() or page_cache_async_readahead() ondemand_readahead() __do_page_cache_readahead() read_pages() mapping-\u003ea_ops-\u003ereadpages(filp, mapping, pages, nr_pages); or mapping-\u003ea_ops-\u003ereadpage(filp, page); /* ext2 defines both readpage() and readpages(). */ const struct address_space_operations ext2_aops = { .readpage = ext2_readpage, .readpages = ext2_readpages, .writepage = ext2_writepage, .write_begin = ext2_write_begin, .write_end = ext2_write_end, .bmap = ext2_bmap, .direct_IO = ext2_direct_IO, .writepages = ext2_writepages, .migratepage = buffer_migrate_page, .is_partially_uptodate = block_is_partially_uptodate, .error_remove_page = generic_error_remove_page, }; /* The IO request is submitted to the block layer by calling mpage_bio_submit(). */ ext2_readpages() mpage_readpages() mpage_bio_submit() /* * __do_page_cache_readahead() actually reads a chunk of disk. It allocates * the pages first, then submits them for I/O. This avoids the very bad * behaviour which would occur if page allocations are causing VM writeback. * We really don't want to intermingle reads and writes like that. * * Returns the number of pages requested, or the maximum amount of I/O allowed. */ unsigned int __do_page_cache_readahead(struct address_space *mapping, struct file *filp, pgoff_t offset, unsigned long nr_to_read, unsigned long lookahead_size) { ... /* * Preallocate as many pages as we will need. */ for (page_idx = 0; page_idx \u003c nr_to_read; page_idx++) { pgoff_t page_offset = offset + page_","date":"2019-01-16","objectID":"/linux-read-path/:0:1","tags":["engineering","linux","source code"],"title":"Read syscall implementation in Linux Kernel","uri":"/linux-read-path/"},{"categories":null,"content":"Amazon RDS PostgreSQL 2 flavors. Open-source PostgreSQL on top of EBS Aurora PostgreSQL on top of Aurora Storage (better performance) Supports Postgre 9.6/10 Aurora Postgres supports one RW master node and many read replicas. A read replica will be promoted to the master node when the original master node fails. Aurora Postgres supports fast clones. Only need to pay for the storage of changed data. Will support logical replication by introducing the logical decoding plugin which converts physical changes to SQL statements. The logical decoding plugin can also be used as the event source for publish/subscribe. Aurora Postgres bypasses filesystem page cache and manages its memory directly (more memory as the shared buffer). Aurora Postgres supports survivable cache: the shared buffer will survive postgres process crashes. This enable faster performance recovery. S3: Simple Storage Service AWS S3 team: ~1000 engineers When a bucket is created, it will be assigned one initial partition. Each partition can support up to 3,500 PUTs or 5,500 GETs. This means you either get 3,500 PUTs and 0 GET tps or 0 PUTs and 5,500 GET tps. 50% PUT and 50% GET = (0.5 * 3,500) + (0.5 * 5,500) = 4,500 tps combined 30% PUT and 70% GET = (0.3 * 3,500) + (0.7 * 5,500) = 4,900 tps combined. In the backend, they detect access load on a partition. When a partition becomes hot, they will split the partition based on object prefix. They do the re-partition only when the load is consistently high for at least half an hour. They do not support auto scale-down for now. ML frameworks MXnet: efficient distributed training; portability; efficient inference; inference on edge Gluon: easy coding; easy debugging; toolkits for rapid prototyping SageMaker: end-2-end platform; zero setup; distributed training; AB/testing; scalable endpoints; automatic model tuning DynamoDB Deep Dive why choose NoSQL? SQL: optimized for storage Normalized/relational Ad hoc queries Scale vertically Good for OLAP NoSQL: optimized for compute Denormalized/hierarchical Instantiated views Scale horizontally Built for OLTP at scale Partition key Large number of distinct values Items are uniformly requested and randomly distributed Sort key Efficient/selective patterns Leverage range queries 1 application service = 1 table Reduce round trips Simplify access patterns Inside Amazon, they have managed to support ~40 different queries with one table. Lambda/Serverless One function per one microVM No orchestration code in a Lambda function. Retry/error handling should be done using Step functions. Should avoid monorepo for functions: one repo per function. This simplifies permissions and keeps each function small (minimize dependency for each function and use less memory). Four stages in the function lifecycle Download your code (cold start) Start new execution environment (cold start) Bootstrap the runtime (Partial cold start) Start your code (Warm start) Use AWS X-Ray for instrumenting Lambda functions CPU is proportional to memory size: increase the memory size will make a function run much faster and in some cases, could save the cost as well. \u003c 1.8 GB: single core 1.8 GB: 2 cores AWS RedShift Column data is stored as 1 MB immutable blocks Blocks are encoded with 1 of 12 encodings. Zone map contains min/max values for each block. Used to eliminate unnessary IOs. Misc choose boring technology How We Massively Reduced Our AWS Lambda Bill with Go ","date":"2018-12-07","objectID":"/reinvent2018-tidbits/:0:0","tags":["conference"],"title":"AWS re:Invent 2018 tidbits","uri":"/reinvent2018-tidbits/"},{"categories":null,"content":"I attended the AWS re:Invent at Las Vegas the week after Thanksgiving. It was a great week there, learning AWS technologies while also having fun. The keynotes given by Andy Jassy and Werner Vogels were always exciting: they covered important new services that were launched during this event. Other sessions shared best practices in using AWS services and infrastructure. They also had music bands playing live music at the site and provided great lunches and snacks during the morning and afternoon breaks. Besides, we can come and pick up a swag every day. This year, more than 50,000 people attended the conference and you can see lots of people walking around. This is by far the largest conference I have ever attended and I was amazed by how well they managed to organize it. Overview AWS annual revenue has grown to 27 billions US dollars, with 47% YoY growth. From the most recent Gartner report, AWS shares 52% market share for the IaaS market. Overall, AWS has ~140 services with 11 database services. Compute Support for ARM processors: A1 instances. Good for scale-out application and bring cost saving (~45%) V100 GPUs: P3 instances with 8 NVIDIA Tesla V100 GPUs. Elastic graphic / Elastic Inference: add GPU on-demand to normal EC2 instances, only for graphic or inference tasks 100 Gpbs NIC: c5n instances Storage: ","date":"2018-12-03","objectID":"/reinvent2018/:0:0","tags":["conference"],"title":"AWS re:Invent 2018","uri":"/reinvent2018/"},{"categories":null,"content":"Simple Storage Service (S3) Intelligent tiering: automatically migrate objects from S3 standard to S3 infrequent access, to save customer S3 storage cost Glacier deep archive: a tape replacement solution. Even cheaper than Glacier. S3 batch operations: apply the same operation for billions of objects. To use this feature, we first provide a list of objects, either by using the S3 inventory report or generating the list by ourselves. Then, we select a desired action from pre-populated menu of options. S3 will do the job, send the progress and notification when the job is completed. ","date":"2018-12-03","objectID":"/reinvent2018/:1:0","tags":["conference"],"title":"AWS re:Invent 2018","uri":"/reinvent2018/"},{"categories":null,"content":"Elastic File Service (EFS) A new class: Infrequent Access. Save cost (~85%). FSx: FSx for Windows file server: native windows FS; fully managed by AWS; can support 10GB/s throughput FSx for Lustre: fully managed; for HPC users ","date":"2018-12-03","objectID":"/reinvent2018/:2:0","tags":["conference"],"title":"AWS re:Invent 2018","uri":"/reinvent2018/"},{"categories":null,"content":"Data Migration Service AWS DataSync Service: migrate data between on-premise NFS server and S3/EFS. Need to deply a VM at on-premise datacenter to run the DataSync agent. Mentioned use of compression and multi-part PUTs to improve the transfer performance. Audiences also asked the support of Object store/SMB/HDFS as another source. New Database services AWS Quantum Ledger Database (QLDB): to store immutable, and cryptographically verifiable transaction logs where there is a central trusted authority. AWS Timestream: time series database AWS Lambda: IDEs: AWS Toolkits for Visual Studio Code, IntelliJ, and PyCharm Supports custom runtime for any Linux compatible language runtime Supports Ruby, C++, and Rust Supports for Php, cobol, Erlang, and elixir are available from AWS partners Lambda layers: allow multiple lambda functions to share libraries AWS serverless application repository: store, share, and deploy serverless applications Step function service integration: from a step function, one can now talk to Lambda, Batch, DynamoDB, ECS/Fargate, SNS, SQS, Glue, and SageMaker. Application load balancer (ALB) support for Lambda: one can trigger lambda functions directly from ALB. Firecracker: AWS’s lightweight virtualization technology based on KVM. It is the underling container technology powering AWS Lambda and AWS Fargate. Each VM takes only 5 MB of memory. They claimed to achieve startup time as short as 125 ms and can provision 150 microVMs per min/host. They make it open source. Machine Learning Three layers of AI software stack (from top to bottom) AI Services: TextToSpeed, etc ML Services: Sagemaker ML frameworks: TensorFlow, MXNet, and pyTorch SageMaker Groundtruth: provides auto labelling or human to add tags to datasets; For auto labelling, one can set the confidence level: it will only set labels when the confidence from the algorithm is higher than the threshold. human label: mechanical trunk; third party; or your friends. AWS Marketplace for machine learning: a place to pick up/sell machine learning packages. SageMaker Reforcement Learning: provides RL models. SageMaker Neo: open-source compiler to compile a machine model for different architectures (TPU/GPU/Intel CPU/ARM, etc.) AWS Inferentia: a machine learning inference chip designed to deliver high performance at low cost; A customized chip by AWS optimized for inference; AWS Outposts Bring AWS hardware to your datacenters; can run AWS software to get consistent management/use experience; AWS’s strategy for hybrid cloud; can also run VMWare Cloud on outposts for VMWare customers. Other new services AWS control tower: provides best practice guides/blueprints; prescribed guides for how to best use AWS infrastruture AWS security hub AWS Lake formation: create data lakes AWS Managed Blockchain AWS Managed Kafka AWS forcast: time series forecasting AWS personalize: real-time personalization and recommendation ","date":"2018-12-03","objectID":"/reinvent2018/:3:0","tags":["conference"],"title":"AWS re:Invent 2018","uri":"/reinvent2018/"},{"categories":null,"content":"It is surprised to come across several systems/services that happen to built on top of postgreSQL. Here is a quick summary of these. Systems/services Type AWS Redshift data warehousing TimeScale time series database openGauss/GaussDB distributed relational db, by Huawei AWS Redshift: Redshift is a data warehousing service offered by AWS. It is built on postgreSQL, with a new columnar storage engine. The use of columnar storage is expected to provide better query performance and better compression. TimeScale: TimeScale DB is a new time series database. It basically is a cluster of postgreSQL databases. Data is partitioned based on data source and time intervals to facilitate faster query processing, with each query accessing local data as much as possible. ","date":"2018-12-02","objectID":"/postgresql/:0:0","tags":["engineering"],"title":"Systems and services built on top of postgreSQL","uri":"/postgresql/"},{"categories":null,"content":"Just out of curiosity, I wanted to know how many lines of code for various software that I am interested in (mostly, file systems and key-value stores). ext4 is the newer version for ext2, mainly added with the journaling mechanism to provide data consistency for crashes. The number of codes for ext4 is 5 times more than ext2. f2fs is a new file system that is optimized for flash devices while nova is a new file system that is optimized for persistent memory. Both of them are in the order of 30,000 lines of code. I also looked at leveldb and rocksdb, which are key-value stores. rocksdb is derived from leveldb and has gained great popularity in recent years. While rocksdb focked from leveldb, as we can see, the number of lines of code in rocksdb is \u003e10 times more than leveldb. Maybe, we should really consider rocksdb as a different key-value store. While key value stores are supposed to be more performant and provide simple APIs, it is interesting to see that rocksdb has grown to be even more complex (4 times more in lines of code) than an actual relational database: postgreSQL. Software Type Lines of Code ext2 file system 10,000 ext4 file system 52,000 nova file system 21,000 f2fs file system 27,500 leveldb key value store 29,000 rocksdb key value store 443,000 postgreSQL relational database 113,000 ","date":"2018-11-20","objectID":"/loc/:0:0","tags":["coding"],"title":"Lines of Code for Various Software","uri":"/loc/"},{"categories":null,"content":"Finally got some time to test out Intel Optane SSD and Samsung ZSSD with SPDK. Once again, I compared performance numbers from SPDK with Linux block device. Here are some numbers. The experiments were run in a Ubuntu14.04 server and we tested 4KiB random read performance. Intel Optane SSD Metric SPDK /dev/nvme1n1 IOPS 150 K 66.1 K slat 0.13 usec 2.8 usec clat 6.2 usec 11.4 usec lat 6.35 usec 14.32 usec Samsung ZSSD Metric SPDK /dev/nvme0n1 IOPS 73.9 K 46.2 K slat 0.14 usec 3.2 usec clat 13.0 usec 17.4 usec lat 13.12 usec 20.73 usec slat: average submission latency clat: the time between submission to the kernel and when the IO is complete lat: total IO latency Fio output: SPDK on Intel Optane SSD xing@atg-s-holder:~/w/spdk/examples/nvme/fio_plugin$ sudo LD_PRELOAD=/home/xing/w/spdk/examples/nvme/fio_plugin/fio_plugin /home/xing/w/fio/fio /home/xing/w/spdk/examples/nvme/fio_plugin/example_config.fio test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=spdk,iodepth=1 fio-3.3 Starting 1 thread Starting SPDK v18.07-pre / DPDK 18.02.0 initialization... [ DPDK EAL parameters: fio -c 0x1 -m 512 --file-prefix=spdk_pid25303 ] EAL: Detected 32 lcore(s) EAL: Multi-process socket /var/run/.spdk_pid25303_unix EAL: Probing VFIO support... EAL: PCI device 0000:81:00.0 on NUMA socket 1 EAL: probe driver: 8086:2701 spdk_nvme Jobs: 1 (f=1): [r(1)][100.0%][r=587MiB/s,w=0KiB/s][r=150k,w=0 IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=25339: Thu May 10 21:47:08 2018 read: IOPS=150k, BW=587MiB/s (615MB/s)(68.8GiB/120000msec) slat (nsec): min=121, max=515336, avg=130.59, stdev=126.80 clat (nsec): min=137, max=821212, avg=6217.80, stdev=1214.79 lat (usec): min=5, max=821, avg= 6.35, stdev= 1.22 clat percentiles (nsec): | 1.00th=[ 5984], 5.00th=[ 6048], 10.00th=[ 6048], 20.00th=[ 6048], | 30.00th=[ 6048], 40.00th=[ 6112], 50.00th=[ 6112], 60.00th=[ 6112], | 70.00th=[ 6176], 80.00th=[ 6176], 90.00th=[ 6240], 95.00th=[ 6304], | 99.00th=[ 7392], 99.50th=[14784], 99.90th=[30592], 99.95th=[33024], | 99.99th=[35072] bw ( KiB/s): min=597485, max=602936, per=99.99%, avg=600786.28, stdev=973.76, samples=239 iops : min=149373, max=150734, avg=150196.54, stdev=243.53, samples=239 lat (nsec) : 250=0.01%, 500=0.01% lat (usec) : 2=0.01%, 4=0.01%, 10=99.29%, 20=0.57%, 50=0.14% lat (usec) : 100=0.01%, 250=0.01%, 1000=0.01% cpu : usr=100.01%, sys=0.07%, ctx=11458, majf=0, minf=3110 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0% issued rwt: total=18026026,0,0, short=0,0,0, dropped=0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): READ: bw=587MiB/s (615MB/s), 587MiB/s-587MiB/s (615MB/s-615MB/s), io=68.8GiB (73.8GB), run=120000-120000msec Linux block device on Intel Optane SSD xing@atg-s-holder:~/w/spdk/examples/nvme/fio_plugin$ sudo ~/w/fio/fio example_config-nvme.fio test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1 fio-3.3 Starting 1 thread Jobs: 1 (f=1): [r(1)][100.0%][r=261MiB/s,w=0KiB/s][r=66.9k,w=0 IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=26385: Thu May 10 21:56:17 2018 read: IOPS=66.1k, BW=258MiB/s (271MB/s)(30.3GiB/120001msec) slat (nsec): min=1836, max=120042, avg=2817.72, stdev=926.54 clat (nsec): min=832, max=190920, avg=11409.63, stdev=1911.75 lat (usec): min=10, max=194, avg=14.32, stdev= 2.33 clat percentiles (nsec): | 1.00th=[10560], 5.00th=[10688], 10.00th=[10816], 20.00th=[10944], | 30.00th=[10944], 40.00th=[11072], 50.00th=[11072], 60.00th=[11200], | 70.00th=[11200], 80.00th=[11456], 90.00th=[11968], 95.00th=[12352], | 99.00th=[23168], 99.50th=[24192], 99.90th=[36608], 99.95th=[39168], | 99.99th=[48384] bw ( KiB/s): min=253128, max=272792, per=100.00%, avg=264530.47, stdev=3153.35, samples=239 iops : min=63282, max=681","date":"2018-05-10","objectID":"/spdk-optane-zssd/:0:0","tags":["SPDK"],"title":"SPDK on Intel Optane and Samsung ZSSD","uri":"/spdk-optane-zssd/"},{"categories":null,"content":" Update packages sudo apt-get update Install php7. The default php coming with Ubuntu 14 is 5.5.9 while hotCRP prefers 5.6 and later. Install the software-properties-common package to get the add-apt-repository command. sudo apt-get install software-properties-common Add php7 ppa and do an update sudo add-apt-repository ppa:ondrej/php sudo apt-get update Install php7 sudo apt-get install php7.2 php7.2-mysql Install other required packages sudo apt-get install git apache2 mysql-server zip poppler-utils sendmail Get hotCRP source code git clone https://github.com/kohler/hotcrp.git Create the database for hotCRP lib/createdb.sh --user root --password Add the read permission for conf/options.conf chmod +r conf/options.conf Add the directory to /etc/apache2/apache2.conf \u003cDirectory \"/w/hotcrp\"\u003e Options Indexes Includes FollowSymLinks AllowOverride all Require all granted \u003c/Directory\u003e Alias /hotcrp /w/hotcrp Set upload_max_filesize, post_max_size, and max_input_vars in hotcrp/.htaccess file. Use default values are good enough. /etc/php/7.2/apache2/php.in Increase the session gc life time session.gc_maxlifetime = 86400 Enable mysqli extension, by uncommenting the following line ;extension=mysqli Restart the Apache web server sudo service apache2 restart The website is up and running. Visit IP_address/hotcrp to check it out. ","date":"2018-04-12","objectID":"/hotcrp-ubuntu14/:0:0","tags":["hotCRP","note"],"title":"Install hotCRP at a Ubuntu 14 Machine","uri":"/hotcrp-ubuntu14/"},{"categories":null,"content":"I got the opportunity to participate in the review process as a program committee member for USENIX ATC 2018. USENIX ATC is a big conference in term of submissions: this year, it received more than 400 complete submissions. To handle such a huge load on reviewing, the chairs had purposedly doubled the number of program committee members to be 70 this year. Still, it requires a careful planning to run the paper reviewing process smoothly. Two rounds reviews. The load is roughly 3 days per paper for each program committee member. Each paper got 2 reviews during the first round and another 2, if a paper entered the second round (as long as a paper got at least one accept, whether it is an accept or a weak accept, it was qualifed to enter the second round). One week buffer time was introduced between the end of the first round and the start of the second round. The primary benefit or goal is to give people some time to turn in late reviews. This is very effective: in average, everyone was supposed to turn in 12 reviews for the first round. However, when the deadline of the first round was past, in average, each PC member had turned in only 4 reviews. The number went to 10 reviews per person by the end of the next week. Deadline is the primary productive force！ Another week was introduced between the end of the second round and the final PC meeting. The primary goal is to give people some time for online discussion, to reach consesuses (pre-accept or pre-reject) before in person meeting. This can reduce the number of papers that need to be discussed at the final PC meeting. Collected those who will come to the PC meeting and assign them as discussion leaders for papers that entered the second round. Right after the second round reviews ended, discussion leaders were assigned and they were encouraged to start online discussions as soon as possible. ","date":"2018-04-11","objectID":"/atc18-best-practice/:0:0","tags":["conference"],"title":"Best Practices for Running the Conference Review Process Based on USENIX ATC 2018","uri":"/atc18-best-practice/"},{"categories":null,"content":"Today, I started to play with Intel SPDK with a 250GB Samsung 960 EVO NVMe SSD. I was thinking that using SPDK might lead to better performance, than through traditional block device interface. However, using fio with 4KiB random read requests at the queue depth equal to 1, as it turned out, the difference is rather small. Metric SPDK /dev/nvme0n1 IOPS 12.8 K 11.6 K slat 0.18 usec – clat 77.73 usec 84.19 usec lat 77.91 usec 84.32 usec slat, clat and lat are average submission latency, submission to IO completion latency and total IO latency. As we can see, this device still have a quite high read latency. The latest Intel Optane DC P4800X SSDs can achieve 10 us read latency, with 550 K IOPS. For this Samsung device, software overhead seems still fine. It would be really interesting to test out for the Intel Optane PC4800X SSDs though. ","date":"2017-12-01","objectID":"/spdk-samsung-960-evo/:0:0","tags":["SPDK"],"title":"SPDK on Samsung 960 EVO NVMe SSD","uri":"/spdk-samsung-960-evo/"},{"categories":null,"content":"Tips Dependencies to install, in order to compile SPDK in debian 8 sudo apt-get install libnuma-dev uuid-dev libaio-dev libcunit1-dev ","date":"2017-12-01","objectID":"/spdk-samsung-960-evo/:1:0","tags":["SPDK"],"title":"SPDK on Samsung 960 EVO NVMe SSD","uri":"/spdk-samsung-960-evo/"},{"categories":null,"content":"Books A Philosophy of Software Design, John Ousterhout, Professor at Stanford University The Innovator’s Dilemma, Clayton Christensen UNIX - A History and a Memorior, Brian Kernighan, Professor at Princeton University HOW NOT TO BE WRONG, Jordan Ellenberg, Professor of Mathematics at the University of Wisconsin-Madison THINKING, FAST AND SLOW, Daniel Kahneman, Professor of Psychology Emeritus at Princeton University China’s Economy, Arthur R. Kroeber ","date":"2015-12-25","objectID":"/books-papers/:1:0","tags":["papers","books"],"title":"Great Books and Papers","uri":"/books-papers/"},{"categories":null,"content":"Papers There are some papers that are so well written that I really enjoyed reading them. Here is a list of them. In Search of an Understandable Consensus Algorithm Decompose the consensus problem into multiple sub-problem and tackle each sub-problem individually Optimizing Space Amplification in RocksDB This paper provides a great introduction on LSM-tree, how RocksDB is used as the storage engine for MySQL and some statistics of three LSM-tree based systems used in production in Facebook. BlueCache: A Scalable Distributed Flash-based Key-value Store The system design is very clear from the paper. Learned a lot about its index and data store part. What’s Really New with NewSQL? Provides a historial view of how we enter into NewSQL and did an excellent survey of state-of-art NewSQL Databases NV-Tree: Reducing Consistency Cost for NVM-based Single Level Systems Excellent analysis of overhead in keeping B+ tree consistent, followed by a clear presentation of the proposed solution. The Multi-Queue Replacement Algorithm for Second Level Buffer Caches Excellent analysis of re-use distance in second-level cache, to motivate their cache design CAFTL: A Content-Aware Flash Translation Layer Enhancing the Lifespan of Flash Memory based Solid State Drives ","date":"2015-12-25","objectID":"/books-papers/:2:0","tags":["papers","books"],"title":"Great Books and Papers","uri":"/books-papers/"},{"categories":null,"content":"This paper introduces a new B+ tree variation that is optimized for NVM by reducing consistency cost from memory fence and cacheline flush operations. Consistency is guaranteed only for leaf nodes. Internal nodes are re-built from leaf nodes. Elements in leaf nodes are not sorted. A log-structure is used for leaf nodes to remove the need for frequent synchronization. Only appends are added to a leaf node. This also increases concurrency. When a leaf node becomes full, a GC is triggered for cleaning and consolidation. Internal nodes are managed using arrays, rather than pointed based data structures. ","date":"2015-12-25","objectID":"/nv-tree/:0:0","tags":["paper"],"title":"NV-Tree: Reducing Consistency Cost for NVM-based Single Level Systems","uri":"/nv-tree/"},{"categories":null,"content":"PhD Dissertation Using Similarity in Content and Access Patterns to Improve Space Efficiency and Performance in Storage Systems [slides] Conference Papers Fantastic SSD Internals and How to Learn and Use Them Nanqinqin Li, Mingzhe Hao, Huaicheng Li, Xing Lin, Tim Emami, and Haryadi S. Gunawi (To appear) SYSTOR 2022: The 15th ACM International Systems and Storage Conference IODA: A Host/Device Co-Design for Strong Predictability Contract on Modern Flash Storage [bibtex] Huaicheng Li, Martin L. Putra, Ronald Shi, Xing Lin, Gregory R. Ganger, and Haryadi S. Gunawi SOSP 21: The 28th ACM Symposium on Operating Systems Principle SEFEE: Lightweight Storage Error Forecasting in Large Scale Enterprise Storage Systems Amirhesam Yazdi, Xing Lin, Lei Yang, Feng Yan SC 2020: International Conference for High Performance Computing, Networking, Storage and Analysis SS-CDC: A Two-stage Parallel Content-Defined Chunking for Deduplicating Backup Storage [slides :: bibtex] Fan Ni, Xing Lin, Song Jiang SYSTOR 2019: The 12th ACM International Systems and Storage Conference Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems [bibtex] Haryadi S. Gunawi and many other collaborators FAST 18: The 16th USENIX Conference on File and Storage Technologies, Feb. 2018 Metadata Considered Harmful … to Deduplication [slides :: bibtex] Xing Lin, Fred Douglis, Jim Li, Xudong Li, Robert Ricci, Stephen Smaldone, and Grant Wallace HotStorage 15: The 7th USENIX Workshop on Hot Topics in Storage and File Systems, July 2015 POTASSIUM: Penetration Testing as a Service [bibtex] Richard Li, Dallin Abendroth, Xing Lin, Yuankai Guo, Hyun-wook Baek, Eric Eide, Robert Ricci, Jacobus Van der Merwe SOCC 15: ACM Symposium on Cloud Computing, August 2015 Using Deduplicating Storage for Efficient Disk Image Deployment [slides :: bibtex] Xing Lin, Mike Hibler, Eric Eide, and Robert Ricci TridentCom 15: The 10th International Conference on Testbeds and Research Infrastructures for the Development of Networks \u0026 Communities, June 2015 Migratory Compression: Coarse-grained Data Reordering to Improve Compressibility [slides :: video :: bibtex] Xing Lin, Guanlin Lu, Fred Douglis, Philip Shilane, and Grant Wallace FAST 14: The 12th USENIX Conference on File and Storage Technologies, Feb. 2014 Towards Fair Sharing of Block Storage in a Multi-tenant Cloud [slides :: video :: bibtex] Xing Lin, Yun Mao, Feifei Li, Robert Ricci HotCloud 12: The 4th USENIX Workshop on Hot Topics in Cloud Computing, June 2012 Refining the Utility Metric for Utility-Based Cache Partitioning [slides :: bibtex] Xing Lin, and Rajeev Balasubramonian WDDD 11: The 9th Annual Workshop on Duplicating, Deconstructing, and Debunking, in conjunction with the 38th International Symposium on Computer Architecture (ISCA-38), June 2011 Journal papers Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems [bibtex] Haryadi S. Gunawi and many other collaborators ACM Transactions on Storage, Volume 14 Issue 3, October 2018, Article No. 23 For a complete list, please see my Google Scholar profile. ","date":"2015-08-19","objectID":"/publications/:0:0","tags":null,"title":"Publications","uri":"/publications/"},{"categories":null,"content":"After six years in the PhD program, since August 2009, I finally defended my dissertation today. I feel grateful that I finally make it. I do think I am one of the luck ones: I have a really nice adviser and he gives me strong support to do research in areas which I am interested in. I feel grateful for all the help I have received from my advisor, committee members, office mates and co-workers. The journey would be much challenging if I did it alone. Thank you all! August 7, 2009: From China to United States I started as a fresh PhD student in August 2009. During that semester, we had 6 people coming from China to join the department as PhD students. Five of us negociated online and we actually took the same flight to come to United States. We stayed overnight at the Los Angeles International Airport before we arrived at our destination: Salt Lake City at Utah. Everyone felt excited when we first arrived. Salt Lake City is beautiful: it has very clear and blue sky and it is surrounded with mountains. After we settled down, our school life started. Year 1-2: Taking Courses During the first two years, we were mainly taking courses. The idea is to finish course requirements as soon as possible so that we can focus on research in our later years. From these courses, we did build more solid understanding about basic concept in computer systems. I had the chance to take the operating system course from Matthew Flatt, the network security course from Sneha Kasera, Computer Architecture from Rajeev Balasubramonian, Data Mining from Jeff Philips, Database Kernels from Feifei Li. Matthew used to write code when he was giving his lectures. I did not like that way by that time (I do appreciate that way now). It was really great to build some relationship with these professors when we were taking their classes. For example, I worked closely with Rajeev when I was taking the Advanced Computer Architecture course and by the end, we were able to submit a paper to a workshop. That paper became the first publication during my PhD study and I got the chance to give the talk and attend ISCA in 2011. During the interaction with these professors, you can also tell which professor you are interested to work with. This is probably the most important issue to figure out when you are in your early stage of PhD study. At the same time, I was also looking for RA positions and Eric Eide did some interviewswith me during an informal lunch, together with my current adviser. Since I had some hacking experience in the Window kernel, they let me to try RA for one semester. For the first semester, they assigned a project for me. It was to use some data collection library so that a user can send their experiemnt data from distributed compute nodes to this data collection node. We were trying to figure out how this data collection library scales, mainly how many nodes can it support. It seemed that this library worked pretty good and we decided to use it by the end. After the first project, I started to discuss the next project with my advisor. I had some experience in process migration and wanted to do some work with virtual machine migration. So, I read lots of papers about virtual machine migration at that time and also worked on OpenVZ migration in Emulab. I also started to write a survey paper about process and virtual machine migration. But we never tried to submit it somewhere. I still remember that it was quite challenging for me to understand a paper by that time. Our group is running a paper reading semester and in most cases, we read one paper every week. Given nearly no research experience in this field, it is really difficult to understand what a paper is about. I still remember that for every week, I spent the whole thursday night to read a paper once, to get farmilar with its work. During the morning on the next day, I need to read it again. This time is to digest this paper and write summary about that paper. This paper reading exercise took almo","date":"2015-07-22","objectID":"/dissertation-defense/:0:0","tags":["PhD life"],"title":"Reflection After Dissertation Defense","uri":"/dissertation-defense/"},{"categories":null,"content":"Tonight, I went to attend a local meetup event. The talk was given by a faculty from my own department, Jeff Phillips, an assistant professor working in the area of big data analysis. The topic of his talk is about sketch data structure - data structures which uses a much smaller amount of memory to summarize the represented data.This is a cool technique and I am very interested to learn more. Sketch data structures are commonly used in the following three use cases. frequent items, like IP addresses distinct items (harder to do) approximate distribution of items And it is now also becoming popular to use sketches to represent matrixes and graphs matrix sketch (hot) graph sketch (new) Here is a list of sketches Jeff talked in his talk. reservoir sampling: keep k samples over a streaming items keep the first k items; for j_th item where j\u003ek, keep it at probability of k/j. Frequent items sketches Assume we want to find top k frequent items Misra-Gries sketch (under-count) initialize k counters = 0 for i_th item if (we have a counter that is not 0 for this item) increase the counter by 1 elsif (we have a counter that is 0) increase the counter by 1 and assign this counter to this item else decrease all counters by 1 spacesaving sketch (over-count) initialize k counters = 0 for i_th item if (we have a counter that is not 0 for this item) increase the counter by 1 else find the counter with minimal value; increase the counter by 1 and assign this counter to this item Two generic sketches to estimate frequency of any item count-min sketch (over-count, report minimal value, support substraction) initialize a two-dimensional t*k array of counters pick t independent hash fuctions for i_th item foreach hash function h increase the counter of h(item) for any item, report the minimal value of h(item) as the estimation of its frequency count sketch initialize a two-dimensional t*k array of counters pick t independent hash fuctions h_1,...,h_t, each maps an item to {1,...,k} pick t independent hash functions s_1,...,s_t, each maps an item to {+1, -1} for i_th item foreach hash function h_i h_i(item) += s_i(item) for any item, report the median value of h(item) as the estimation of its frequency Count sketch: “Finding Frequent Items in Data Streams”, Moses Charikar, Kevin Chen and Martin Farach-Colton, ICALP ‘02 Proceedings of the 29th International Colloquium on Automata, Languages and Programming. ","date":"2015-06-09","objectID":"/sketch-data-structures/:0:0","tags":["technique","data summarization"],"title":"Sketch Data Structures","uri":"/sketch-data-structures/"},{"categories":null,"content":" List storage pools virsh pool-list --details List all volumes for a pool vol-list --details POOLNAME To create a Qemu guest with a image in qcow2 format, we need to first create a storage pool first. virsh pool-define-as --name qcow2 --type dir --target /mnt/qemu-img virsh pool-start qcow2 To create a Qemu guest with bridged network sudo virt-install -n vm1 -r 1024 --disk path=/mnt/qemu-imgs/vm1.qcow2,bus=virtio,size=5,format=qcow2 -c /mnt/sda4/ubuntu-12.04.5-desktop-amd64.iso --network bridge=virbr0 --graphics vnc,listen=0.0.0.0 --noautoconsole -v To create a qemu guest with qemu_system-x86_64 binary directly qemu-system-x86_64 -enable-kvm -m 1024 -name vm2 -hda /var/lib/libvirt/images/vm2.img -vnc :2 -device e1000,netdev=net1,mac=22:22:33:44:00:02 -netdev tap,id=net1 -cdrom /mnt/sdb1/ubuntu-14.04.1-desktop-i386.iso -boot d To get console access for guest VMs created with virt-install Create a guest VM sudo virt-install -n vm1 -r 256 --disk path=/var/lib/libvirt/images/vm1.img,bus=virtio,size=10 -c ubuntu-14.04-server-i386.iso --network network=default,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole -v Added console to kernel parameter, by editing the file /etc/default/grub and added “console=ttyS0” to the variable GRUB_CMDLINE_LINUX_DEFAULT. Create /etc/init/ttyS0.conf and change last line to be “exec /sbin/getty -8 38400 ttyS0 xterm” cp /etc/init/tty1.conf /etc/init/ttyS0.conf +exec /sbin/getty -8 38400 ttyS0 xterm ","date":"2015-04-08","objectID":"/kvm/:0:0","tags":["KVM"],"title":"KVM Notes","uri":"/kvm/"},{"categories":null,"content":"I did my PhD proposal denfense yesterday and now I am one more step closer to finish it. It took a week to just prepare the talk. The talk was long and have 45 slides. However, there are only about 8 slides for each project, since there are 4 projects. The talk lasts about 45 minutes. Because it is so long, everytime I practiced it, I almost lost my voice after I finished. We used a new system that can connect skype and regular phone and we can hear everyone clearly. However, background noise came from time to time and this interrupted my talk several times (quite annoying). During the talk, members in my committees asked many questions. This is probably the best opportunity for them to ask whatever questions they have. And I am glad they did that. For some questions, I did not think I answered them well. The good thing is that most of work was done in collaboration with one of the committee members and they joined in, in helping me to answer when my answer was not satisfying or convincing.I should improve in this aspect. After the talk, we had some discussion. Then, my advisor was proposing to have a private discussion (without me) while Rajeev said there was nothing that was confidencial. And thus, they continued the discussion while I was still sitting there. By the end, they agreed to let me pass the proposal defense. Thanks everyone in my committee. For the dissertation, there are several measurements need to be done and the majority will be dissertation writing. Looking forward, there are lots of work need to be done. We are planning for a USENIX ATC submission by the end of Jan. and a TridentCom submission in the middle of Feb. Happy Christmas and then keep hard-working! ","date":"2014-12-19","objectID":"/proposal-defense/:0:0","tags":["phd life"],"title":"Proposal Defense","uri":"/proposal-defense/"},{"categories":null,"content":"I am working on virtual machine cloning for security testing for the TCloud project. We are looking for an efficient and transparent virtual machine cloning technique that has minimal impact on the virtual machine to be cloned. After some investigation, we found that KVM supports live snapshot for both memory and disk state while Xen4.4 currently does not support this feature yet but could be patched to do live snapshot for memory state easily. btrfs has very impressive performance in taking snapshots: its performance is the best when compared with qemu-img and lvm (finished in about 50 ms for a 4GB disk file). So, we could possibly use both Xen or KVM for our project. The other requirement from the project is we want to provide this security testing as a cloud service. So, we want to use OpenStack as our cloud platform. We had a really hard time in setting up a Xen+libvirt OpenStack cluster and we finally gave up on this approach since it is not supported well yet (in the group C support). As far as I know, there is no online documentation on this topic, until I wrote this one. Because of all the difficulties in setting up a Xen+libvirt OpenStck cluster, we decided to go with KVM. Here is how we can clone a KVM virtual machine and restart it at another machine. Take a live snapshot at the source machine snapshot-create-as vm1 snapshot1 \"firt snapshot for vm1\" --live --memspec file=/dev/shm/vm1.memsnapshot Transfer the memory image, disk file, and configuration file to the destination machine scp /dev/shm/vm1.memsnapshot dest:/mnt/ scp /etc/libvirt/qemu/vm1.xml dest:/etc/libvirt/qemu/vm1.xml scp /var/lib/libvirt/images/vm1.img dest:/var/lib/libvirt/images/vm1.img Restart(Restore) vm1 at destination machine virsh restore /mnt/vm1.memsnapshot ","date":"2014-11-26","objectID":"/clone-kvm/:0:0","tags":["KVM","live clone"],"title":"Clone a Single KVM/qemu Virtual Machine","uri":"/clone-kvm/"},{"categories":null,"content":"During the weekly technical forum within the NetApp ATG group, a team member presented the trip report for OSDI ‘14 this year. In his talk, he mentioned a paper that pushed me to think more in this line. The paper is about how to estimate the working set of a workload, using less memory and runs much faster. The only related work that dealt with the same problem was proposed more than 10 years ago and in that work, the authors tried to get the exact working set size, with a very high demand for memory and runtime. So, in their work, they tried to estimate it, with a goal to reduce memory demand and runtime. They got very impressive results. Together with other papers, I suddenly realized that it is actually quite common in compute science community, to take this approach: when it is too expensive to get the accurate result, it is usually a good time to think about how to approximate it with much lower overhead. There are a few examples, pop up in my mind. One is the use of bloom filter in tracking unique chunks for deduplication. The memory consumption is too high to keep track of every unique chunks. So, bloom filters are used, to keep track of memberships, with a low probability of introducing duplicates. Sparse Indexing is another example: a full chunk index is not scalable as data size increases. Thus, instead of deduplicating against the full chunk index, they only deduplicate new data blocks with a few data blocks that are most similar. There are probably more such projects that I can not remember all of them. 12/16/2014: found another example - BlinkDB. It trade-offs query accurary for response time, and thus enables interactive queries over massive data by running queries on sampled data and presenting results with annotated error bars. 02/21/2015: add another example - MRC estimation. This is another piece of work, trying to approximate miss rate curve, with low memory consumption and runs quickly. The key idea is again to do sampling! It samples a subset of IO requests and uses re-use distances from this subset as an approximation for an application. ","date":"2014-11-26","objectID":"/accurate-approximate-tradeoff/:0:0","tags":["methodology"],"title":"Trade-off Between Accuracy and Runtime Overhead","uri":"/accurate-approximate-tradeoff/"},{"categories":null,"content":"Hi, my name is Xing Lin. I am a big data infrastructure engineer at Linkedin since 2021. I was an ATG (Advanced Technology Group) engineer at NetApp between 2015 and 2020. Before that, I was a PHD student in the school of computing at the University of Utah. My advisor is Prof. Robert Ricci. I did two internships with Fred Douglis in the Advanced Development Team, EMC Backup and Recovery System Division and one internship with Gokul Soundararajan and Jingxin Feng in NetApp ATG. I completed my Bachelor from the College of Software, Nankai University, China in 2009. I am interested in the following areas: big data systems, data deduplication and compression, distributed storage, solid state drives, and key-value stores. In my spare time, I like reading books (especially history and biography), hiking around in the local area, and sampling nice food. The key is deliberative practice: not just doing it again and again, but challenging yourself with a task that is just beyond your current ability, trying it, analyzing your performance while and after doing it, and correcting any mistakes. – Peter Norvig We choose to go to the Moon! Not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win, and the others, too. –John F. Kennedy ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"Home","uri":"/about/"}]